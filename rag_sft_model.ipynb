{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fa547ec-2645-422d-9cc6-0b35a7484620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rag_with_my_model.py\n",
    "import torch\n",
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "import sentencepiece as spm\n",
    "from decoder_only_gpt import My_GPT_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ce9eb38-1362-4944-8a34-0873cbc99c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 1. Load tokenizer & model (tera trained model)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"hindi_tokenizer_new.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a7eb58e-da98-4155-8dd3-18cf03f4f206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "My_GPT_model(\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(32768, 512)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x Decoder_GPT_Block(\n",
       "        (swi_glu): SwiGLU_FFN(\n",
       "          (w1): Linear(in_features=512, out_features=1536, bias=False)\n",
       "          (w2): Linear(in_features=512, out_features=1536, bias=False)\n",
       "          (w3): Linear(in_features=1536, out_features=512, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (masked_mha): Masked_MHA(\n",
       "          (Q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (K): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (V): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (rms_norm0): RMSNorm()\n",
       "        (rms_norm1): RMSNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = My_GPT_model(\n",
    "    vocab_size=sp.get_piece_size(),\n",
    "    num_layers=12,\n",
    "    d_model=512,\n",
    "    d_ff=2048,\n",
    "    num_heads=8,\n",
    "    seq_len=512\n",
    ").to(DEVICE)\n",
    "\n",
    "# Load final SFT checkpoint\n",
    "model.load_state_dict(torch.load(\"full_sft_final.pt\", map_location=DEVICE))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "013919bd-1ea5-4987-a2ab-a22d3c64d354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load embedding & reranker\n",
    "embed_model = SentenceTransformer(\"intfloat/multilingual-e5-base\")\n",
    "reranker = CrossEncoder(\"cross-encoder/mmarco-mMiniLMv2-L12-H384-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0415c7c-723f-4dec-be03-742fa7285e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size from tokenizer: 32768\n",
      "Model vocab size: 32768\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocab size from tokenizer:\", sp.get_piece_size())\n",
    "print(\"Model vocab size:\", model.lm_head.out_features)  # should match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c397434-ca2f-4d2a-aca4-6c226db8b379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Clean text function (tera previous wala use kar sakta hai)\n",
    "def clean_hindi_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    text = re.sub(r'[⁇�]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fcb7090-10dd-4698-a755-db802d8f9361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Chunking function (sentence level)\n",
    "def chunk_text(text, max_tokens=250, overlap=50):\n",
    "    tokens = sp.encode(text)\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        chunk = tokens[i:i + max_tokens]\n",
    "        chunks.append(sp.decode(chunk))\n",
    "        i += max_tokens - overlap\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f49c9ad0-a24c-4dfc-b012-ec66465f7c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Build FAISS index\n",
    "def build_index(chunks):\n",
    "    texts = [\"passage: \" + chunk for chunk in chunks]\n",
    "    embeddings = embed_model.encode(texts, normalize_embeddings=True, batch_size=32)\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(embeddings.astype(np.float32))\n",
    "    return index, chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "153dd73d-f8d4-4c8d-b474-cac0a32c18a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Retrieve + rerank\n",
    "def retrieve(question, index, chunks, top_k=8, rerank_k=3, min_score=0.55):\n",
    "    q_emb = embed_model.encode([\"query: \" + question], normalize_embeddings=True)\n",
    "    scores, idxs = index.search(q_emb, top_k)\n",
    "    \n",
    "    candidates = []\n",
    "    for i, score in zip(idxs[0], scores[0]):\n",
    "        if score >= min_score:\n",
    "            candidates.append(chunks[i])\n",
    "    \n",
    "    if not candidates:\n",
    "        return []\n",
    "    \n",
    "    # Rerank\n",
    "    pairs = [(question, c) for c in candidates]\n",
    "    rerank_scores = reranker.predict(pairs)\n",
    "    ranked = sorted(zip(candidates, rerank_scores), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    context_texts = [text for text, _ in ranked[:rerank_k]]\n",
    "    \n",
    "    # Tokenize context\n",
    "    context_ids = []\n",
    "    for t in context_texts:\n",
    "        context_ids += sp.encode(f\"[संदर्भ] {t}\\n\")\n",
    "    \n",
    "    return context_ids[:300]  # limit for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "14d0be06-4d65-4b7e-a178-a9b11f355a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Prompt builder (strict)\n",
    "def build_prompt(context_ids, question):\n",
    "    instruction = \"\"\"\n",
    "सिर्फ़ नीचे दिए संदर्भ से ही जवाब दो।\n",
    "संदर्भ से बाहर एक शब्द भी मत लिखो।\n",
    "उत्तर 1-2 वाक्य से ज्यादा मत बनाओ।\n",
    "अगर संदर्भ में जवाब नहीं तो सिर्फ़ लिखो: \"संदर्भ में जानकारी नहीं है।\"\n",
    "\"\"\"\n",
    "    prompt_ids = (\n",
    "        sp.encode(instruction + \"\\nसंदर्भ:\\n\") +\n",
    "        context_ids +\n",
    "        sp.encode(f\"\\nप्रश्न: {question}\\nउत्तर:\")\n",
    "    )\n",
    "    return prompt_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9de18c27-20c9-4459-a09f-144093aa236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(prompt_ids, max_new_tokens=80, temperature=0.6, top_p=0.85, repetition_penalty=1.4):\n",
    "    input_ids = torch.tensor([prompt_ids], dtype=torch.long, device=DEVICE)\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(input_ids)[:, -1, :]\n",
    "        \n",
    "        # Repetition penalty\n",
    "        if repetition_penalty != 1.0:\n",
    "            recent = input_ids[0, -64:].tolist()\n",
    "            for tid in set(recent):\n",
    "                logits[0, tid] /= repetition_penalty\n",
    "        \n",
    "        logits = logits / temperature\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "        \n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = False\n",
    "        sorted_probs[sorted_indices_to_remove] = 0.0\n",
    "        \n",
    "        if sorted_probs.sum() == 0:\n",
    "            next_token = torch.argmax(probs, dim=-1).unsqueeze(0)\n",
    "        else:\n",
    "            sorted_probs /= sorted_probs.sum(dim=-1, keepdim=True)\n",
    "            next_token = torch.multinomial(sorted_probs, num_samples=1)\n",
    "            next_token = sorted_idx.gather(-1, next_token)\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "        \n",
    "        if next_token.item() == sp.eos_id():\n",
    "            break\n",
    "    \n",
    "    generated_ids = input_ids[0, len(prompt_ids):].tolist()\n",
    "    return sp.decode(generated_ids).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "36048b19-006c-4625-bea3-e57aaa82a1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Main RAG function\n",
    "def rag_on_pdf(pdf_path, question):\n",
    "    # PDF load + clean + chunk (tera code se)\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    doc.close()\n",
    "    clean_text = clean_hindi_text(text)\n",
    "    chunks = chunk_text(clean_text, max_tokens=250, overlap=50)\n",
    "\n",
    "    # Embed + FAISS index\n",
    "    index, chunks = build_index(chunks)  # tera function\n",
    "\n",
    "    # Retrieve context\n",
    "    context_ids = retrieve(question, index, chunks, top_k=8, rerank_k=3, min_score=0.55)\n",
    "\n",
    "    if not context_ids:\n",
    "        return \"संदर्भ में जानकारी नहीं है।\"\n",
    "\n",
    "    # Strict prompt\n",
    "    instruction = \"\"\"\n",
    "सिर्फ़ नीचे दिए संदर्भ से ही जवाब दो।\n",
    "संदर्भ से बाहर एक शब्द भी मत लिखो।\n",
    "उत्तर 1-2 वाक्य से ज्यादा मत बनाओ।\n",
    "अगर संदर्भ में जवाब नहीं तो सिर्फ़ लिखो: \"संदर्भ में जानकारी नहीं है।\"\n",
    "\"\"\"\n",
    "    prompt_ids = (\n",
    "        sp.encode(instruction + \"\\nसंदर्भ:\\n\") +\n",
    "        context_ids +\n",
    "        sp.encode(f\"\\nप्रश्न: {question}\\nउत्तर:\")\n",
    "    )\n",
    "\n",
    "    # Generate with new function\n",
    "    answer = generate(prompt_ids, max_new_tokens=60, temperature=0.5, top_p=0.8, repetition_penalty=1.6)\n",
    "\n",
    "    return answer.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fbaee1d0-77dc-419f-b1cb-99c6738c35ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: ⁇ 19980 ⁇  9 ⁇ 19 ⁇  003 ⁇  99 ⁇   ⁇  9 ⁇  9 ⁇  9 ⁇  9 ⁇  9 ⁇  9 ⁇  9 ⁇  9 ⁇  9 ⁇  9 ⁇  9 ⁇  9 ⁇  9 ⁇  9 ⁇  9 ⁇  9 ⁇  9 ⁇  9 ⁇  9 ⁇\n"
     ]
    }
   ],
   "source": [
    "# Example run\n",
    "pdf_file = \"gandhi.pdf\"  # ya jo bhi PDF hai\n",
    "question = \"महात्मा गांधी का जन्म कब हुआ था?\"\n",
    "\n",
    "answer = rag_on_pdf(pdf_file, question)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1644cdad-35ee-4b7a-89d1-f6165456f5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs: [5209, 1187, 40, 1424, 2704, 2227, 12589, 28948, 28940, 28, 382, 132, 28869]\n",
      "Decoded: महात्मा गांधी का जन्म 2 अक्टूबर 1869 को हुआ था।\n"
     ]
    }
   ],
   "source": [
    "test_text = \"महात्मा गांधी का जन्म 2 अक्टूबर 1869 को हुआ था।\"\n",
    "ids = sp.encode(test_text)\n",
    "print(\"IDs:\", ids)\n",
    "decoded = sp.decode(ids)\n",
    "print(\"Decoded:\", decoded)  # exact same text aana chahiye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b512fd31-8f67-497c-b2ab-228f137168c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: ⁇ । ⁇ । ⁇ । ⁇ । ⁇ । ⁇ । ⁇ । ⁇ । ⁇ । ⁇ । ⁇ । ⁇ । ⁇ । ⁇ । ⁇\n"
     ]
    }
   ],
   "source": [
    "prompt_text = \"महात्मा गांधी का जन्म \"\n",
    "prompt_ids = sp.encode(prompt_text)\n",
    "input_ids = torch.tensor([prompt_ids], device=DEVICE)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for _ in range(30):\n",
    "        logits = model(input_ids)\n",
    "        next_token = torch.argmax(logits[:, -1, :], dim=-1).unsqueeze(0)\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "        if next_token.item() == sp.eos_id():\n",
    "            break\n",
    "\n",
    "generated = sp.decode(input_ids[0].tolist())\n",
    "print(\"Generated:\", generated[len(prompt_text):].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22beae4e-d31c-40d0-89aa-6b5e28706139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug_rag_pipeline.py\n",
    "import torch\n",
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "import sentencepiece as spm\n",
    "from decoder_only_gpt import My_GPT_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16becae9-5bec-4dfc-ad25-9252788d055c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8321b43c-6d89-4699-8db7-d815e458fd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 1: Loading tokenizer & model ===\n",
      "Tokenizer loaded. Vocab size: 32768\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 1. Load tokenizer & model\n",
    "print(\"\\n=== Step 1: Loading tokenizer & model ===\")\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"hindi_tokenizer_new.model\")\n",
    "print(\"Tokenizer loaded. Vocab size:\", sp.get_piece_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca052a90-452b-4b7f-a55d-8094b9fb6d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SFT checkpoint...\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "model = My_GPT_model(\n",
    "    vocab_size=sp.get_piece_size(),\n",
    "    num_layers=12,\n",
    "    d_model=512,\n",
    "    d_ff=2048,\n",
    "    num_heads=8,\n",
    "    seq_len=512\n",
    ").to(DEVICE)\n",
    "\n",
    "print(\"Loading SFT checkpoint...\")\n",
    "model.load_state_dict(torch.load(\"full_sft_final.pt\", map_location=DEVICE))\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b4207ec2-9746-4ae7-837c-e3d1b1216f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "My_GPT_model(\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(32768, 512)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x Decoder_GPT_Block(\n",
       "        (swi_glu): SwiGLU_FFN(\n",
       "          (w1): Linear(in_features=512, out_features=1536, bias=False)\n",
       "          (w2): Linear(in_features=512, out_features=1536, bias=False)\n",
       "          (w3): Linear(in_features=1536, out_features=512, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (masked_mha): Masked_MHA(\n",
       "          (Q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (K): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (V): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (rms_norm0): RMSNorm()\n",
       "        (rms_norm1): RMSNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18d6385e-92bf-485a-8d47-2178df54e00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 053ab8c7-d83b-4176-937a-666faf2cfa97)')' thrown while requesting HEAD https://huggingface.co/intfloat/multilingual-e5-base/resolve/main/modules.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 2: Loading embedder & reranker ===\n",
      "Embedder & reranker loaded!\n"
     ]
    }
   ],
   "source": [
    "# 2. Load embedding & reranker\n",
    "print(\"\\n=== Step 2: Loading embedder & reranker ===\")\n",
    "embed_model = SentenceTransformer(\"intfloat/multilingual-e5-base\", device=DEVICE)\n",
    "reranker = CrossEncoder(\"cross-encoder/mmarco-mMiniLMv2-L12-H384-v1\", device=DEVICE)\n",
    "print(\"Embedder & reranker loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "632f79fd-166b-4bac-b906-2848d172eabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hindi_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    # Remove non-printable characters\n",
    "    text = re.sub(r'[\\x00-\\x1F\\x7F]', ' ', text)\n",
    "\n",
    "    # Fix common PDF junk chars\n",
    "    text = re.sub(r'[�•ﬁﬂ–—]', ' ', text)\n",
    "\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8576fc5f-6541-48bb-9295-810c21d313be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'महात्मा गांधी का जन्म कब हुआ था?'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl = clean_hindi_text(\"महात्मा गांधी का जन्म कब हुआ था?\")\n",
    "cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "173deb05-c2aa-4f26-84a0-ee337f37ae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Chunk text\n",
    "def chunk_text(text, max_tokens=250, overlap=50):\n",
    "    tokens = sp.encode(text)\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        chunk = tokens[i:i + max_tokens]\n",
    "        chunks.append(sp.decode(chunk))\n",
    "        i += max_tokens - overlap\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92f9a412-2732-4afe-b59f-b458d5e4dfc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['महात्मा गांधी का जन्म कब हुआ था?']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct = chunk_text(\"महात्मा गांधी का जन्म कब हुआ था?\")\n",
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db599114-dac8-450a-9bda-9aec9fd4ef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(chunks):\n",
    "    print(\"\\n=== Step 5: Building FAISS index ===\")\n",
    "\n",
    "    clean_chunks = [c for c in chunks if len(c.strip()) > 20]\n",
    "    print(f\"Filtered chunks: {len(clean_chunks)} / {len(chunks)}\")\n",
    "\n",
    "    texts = [\"passage: \" + c for c in clean_chunks]\n",
    "    embeddings = embed_model.encode(\n",
    "        texts,\n",
    "        normalize_embeddings=True,\n",
    "        batch_size=32,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(embeddings.astype(np.float32))\n",
    "\n",
    "    print(\"FAISS index built!\")\n",
    "    return index, clean_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ed07162-44d7-457f-b72c-2223d22a0adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(question, index, chunks, top_k=8, rerank_k=3, min_score=0.35):\n",
    "    print(\"\\n=== Step 6: Retrieving context ===\")\n",
    "    print(f\"Question: {question}\")\n",
    "\n",
    "    q_emb = embed_model.encode(\n",
    "        [\"query: \" + question],\n",
    "        normalize_embeddings=True\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    scores, idxs = index.search(q_emb, top_k)\n",
    "\n",
    "    candidates = []\n",
    "    for i, score in zip(idxs[0], scores[0]):\n",
    "        if score >= min_score:\n",
    "            candidates.append(chunks[i])\n",
    "\n",
    "    if not candidates:\n",
    "        print(\"No candidates above threshold, fallback to top-2\")\n",
    "        candidates = [chunks[i] for i in idxs[0][:2]]\n",
    "\n",
    "    # Rerank\n",
    "    pairs = [(question, c[:512]) for c in candidates]\n",
    "    rerank_scores = reranker.predict(pairs)\n",
    "\n",
    "    ranked = sorted(\n",
    "        zip(candidates, rerank_scores),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    context_texts = [t for t, _ in ranked[:rerank_k]]\n",
    "\n",
    "    # Pack context safely\n",
    "    MAX_CONTEXT_TOKENS = 350\n",
    "    context_ids = []\n",
    "    for t in context_texts:\n",
    "        ids = sp.encode(f\"[संदर्भ] {t}\\n\")\n",
    "        if len(context_ids) + len(ids) > MAX_CONTEXT_TOKENS:\n",
    "            break\n",
    "        context_ids.extend(ids)\n",
    "\n",
    "    print(f\"Context tokens: {len(context_ids)}\")\n",
    "    return context_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bc71b687-15be-44c0-a2c8-139f5104d293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(context_ids, question):\n",
    "    print(\"\\n=== Step 7: Building prompt ===\")\n",
    "\n",
    "    instruction = (\n",
    "        \"सिर्फ़ नीचे दिए संदर्भ से ही जवाब दो।\\n\"\n",
    "        \"संदर्भ से बाहर एक शब्द भी मत लिखो।\\n\"\n",
    "        \"उत्तर 1-2 वाक्य से ज्यादा मत बनाओ।\\n\"\n",
    "        \"अगर संदर्भ में जवाब नहीं तो सिर्फ़ लिखो: \\\"संदर्भ में जानकारी नहीं है।\\\"\"\n",
    "    )\n",
    "\n",
    "    prompt_ids = (\n",
    "        sp.encode(instruction + \"\\n\\nसंदर्भ:\\n\") +\n",
    "        context_ids +\n",
    "        sp.encode(f\"\\nप्रश्न: {question}\\nउत्तर:\")\n",
    "    )\n",
    "\n",
    "    if len(prompt_ids) > 512:\n",
    "        print(\"⚠️ Prompt too long, truncating\")\n",
    "        prompt_ids = prompt_ids[-512:]\n",
    "\n",
    "    print(f\"Prompt length: {len(prompt_ids)} tokens\")\n",
    "    return prompt_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "104dcecc-f281-4dae-88ed-ed3fd417a765",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(prompt_ids, max_new_tokens=80, temperature=0.6, top_p=0.85, repetition_penalty=1.4):\n",
    "    print(\"\\n=== Step 8: Generating answer ===\")\n",
    "    input_ids = torch.tensor([prompt_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    generated_tokens = []\n",
    "    END_ID = sp.piece_to_id(\"<END_ANSWER>\")\n",
    "\n",
    "    for step in range(max_new_tokens):\n",
    "\n",
    "        if input_ids.size(1) > 512:\n",
    "            input_ids = input_ids[:, -512:]\n",
    "\n",
    "        logits = model(input_ids)[:, -1, :]\n",
    "\n",
    "        if repetition_penalty != 1.0:\n",
    "            recent = input_ids[0, -64:].tolist()\n",
    "            for tid in set(recent):\n",
    "                if logits[0, tid] > 0:\n",
    "                    logits[0, tid] /= repetition_penalty\n",
    "                else:\n",
    "                    logits[0, tid] *= repetition_penalty\n",
    "\n",
    "        logits = logits / temperature\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = False\n",
    "        sorted_probs[sorted_indices_to_remove] = 0.0\n",
    "\n",
    "        if torch.isnan(sorted_probs).any() or sorted_probs.sum() <= 0:\n",
    "            next_token = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "        else:\n",
    "            sorted_probs /= sorted_probs.sum(dim=-1, keepdim=True)\n",
    "            next_token = torch.multinomial(sorted_probs, num_samples=1)\n",
    "            next_token = sorted_idx.gather(-1, next_token)\n",
    "\n",
    "        tid = next_token.item()\n",
    "        generated_tokens.append(tid)\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "\n",
    "        if tid == sp.eos_id() or tid == END_ID:\n",
    "            break\n",
    "\n",
    "    answer = sp.decode(generated_tokens).strip()\n",
    "    print(\"Raw generated:\", answer)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ebd2ff6c-c9be-4ffa-90c0-9587de477c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_on_pdf(pdf_path, question):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"QUESTION: {question}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Load PDF\n",
    "    print(\"\\nLoading PDF...\")\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text() + \"\\n\"\n",
    "    doc.close()\n",
    "\n",
    "    print(f\"Raw text length: {len(text)} chars\")\n",
    "\n",
    "    # Clean\n",
    "    clean_text = clean_hindi_text(text)\n",
    "    print(f\"Clean text length: {len(clean_text)} chars\")\n",
    "\n",
    "    if len(clean_text) < 100:\n",
    "        return \"PDF में पर्याप्त सामग्री नहीं है।\"\n",
    "\n",
    "    # Chunk\n",
    "    chunks = chunk_text(clean_text)\n",
    "    print(f\"Number of chunks: {len(chunks)}\")\n",
    "    print(\"Sample chunk:\", chunks[0][:300] + \"...\" if chunks else \"No chunks\")\n",
    "\n",
    "    # Build index\n",
    "    index, chunks = build_index(chunks)\n",
    "\n",
    "    # Retrieve\n",
    "    context_ids = retrieve(question, index, chunks)\n",
    "    if not context_ids:\n",
    "        return \"संदर्भ में जानकारी नहीं है।\"\n",
    "\n",
    "    # Prompt\n",
    "    prompt_ids = build_prompt(context_ids, question)\n",
    "\n",
    "    # Generate\n",
    "    answer = generate(prompt_ids)\n",
    "\n",
    "    print(\"\\nFinal Answer:\", answer)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e1ec24b7-2c4e-4577-8306-15ff3a7ccd00",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'answer_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43manswer_ids\u001b[49m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m([sp\u001b[38;5;241m.\u001b[39mid_to_piece(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m answer_ids])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'answer_ids' is not defined"
     ]
    }
   ],
   "source": [
    "print(answer_ids)\n",
    "print([sp.id_to_piece(i) for i in answer_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "96667b7b-30d8-4caa-ae30-4e71cac556e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "QUESTION: महात्मा गांधी का जन्म कब हुआ था?\n",
      "============================================================\n",
      "\n",
      "Loading PDF...\n",
      "Raw text length: 3162 chars\n",
      "Clean text length: 2361 chars\n",
      "Number of chunks: 4\n",
      "Sample chunk: महात्मा गांधी की जीवनी महात्मा गांधी, जिन्हेंबापूया राष्ट्रपिता कहा जाता है, भारतकेस्वतंत्रता संग्रामकेसबसेप्रमुखनेता थे।उनका पूरा नाम मोहनदासकरमचंदगांधी था। जन्मऔरप्रारंभिकजीवन महात्मा गांधी का जन्म2 अक्टूबर1869 को गुजरातकेपोरबंदरमेंहुआथा।उनकेपिता करमचंदगांधी पोरबंदरराज्य केदीवानथेऔरमाता पुतलीबाईधा...\n",
      "\n",
      "=== Step 5: Building FAISS index ===\n",
      "Filtered chunks: 4 / 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c32cab004174bc3884b13a13112d801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built!\n",
      "\n",
      "=== Step 6: Retrieving context ===\n",
      "Question: महात्मा गांधी का जन्म कब हुआ था?\n",
      "Context tokens: 258\n",
      "\n",
      "=== Step 7: Building prompt ===\n",
      "Prompt length: 335 tokens\n",
      "\n",
      "=== Step 8: Generating answer ===\n",
      "Raw generated: ⁇\n",
      "\n",
      "Final Answer: ⁇\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "============================================================\n",
      "QUESTION: उनके पिता का नाम क्या था?\n",
      "============================================================\n",
      "\n",
      "Loading PDF...\n",
      "Raw text length: 3162 chars\n",
      "Clean text length: 2361 chars\n",
      "Number of chunks: 4\n",
      "Sample chunk: महात्मा गांधी की जीवनी महात्मा गांधी, जिन्हेंबापूया राष्ट्रपिता कहा जाता है, भारतकेस्वतंत्रता संग्रामकेसबसेप्रमुखनेता थे।उनका पूरा नाम मोहनदासकरमचंदगांधी था। जन्मऔरप्रारंभिकजीवन महात्मा गांधी का जन्म2 अक्टूबर1869 को गुजरातकेपोरबंदरमेंहुआथा।उनकेपिता करमचंदगांधी पोरबंदरराज्य केदीवानथेऔरमाता पुतलीबाईधा...\n",
      "\n",
      "=== Step 5: Building FAISS index ===\n",
      "Filtered chunks: 4 / 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e07382e90148668326b724f9a83b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built!\n",
      "\n",
      "=== Step 6: Retrieving context ===\n",
      "Question: उनके पिता का नाम क्या था?\n",
      "Context tokens: 258\n",
      "\n",
      "=== Step 7: Building prompt ===\n",
      "Prompt length: 334 tokens\n",
      "\n",
      "=== Step 8: Generating answer ===\n",
      "Raw generated: ⁇\n",
      "\n",
      "Final Answer: ⁇\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "============================================================\n",
      "QUESTION: गांधीजी की शादी किससे हुई थी?\n",
      "============================================================\n",
      "\n",
      "Loading PDF...\n",
      "Raw text length: 3162 chars\n",
      "Clean text length: 2361 chars\n",
      "Number of chunks: 4\n",
      "Sample chunk: महात्मा गांधी की जीवनी महात्मा गांधी, जिन्हेंबापूया राष्ट्रपिता कहा जाता है, भारतकेस्वतंत्रता संग्रामकेसबसेप्रमुखनेता थे।उनका पूरा नाम मोहनदासकरमचंदगांधी था। जन्मऔरप्रारंभिकजीवन महात्मा गांधी का जन्म2 अक्टूबर1869 को गुजरातकेपोरबंदरमेंहुआथा।उनकेपिता करमचंदगांधी पोरबंदरराज्य केदीवानथेऔरमाता पुतलीबाईधा...\n",
      "\n",
      "=== Step 5: Building FAISS index ===\n",
      "Filtered chunks: 4 / 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8d14bc014b45b09dbd5d1790cfbb76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built!\n",
      "\n",
      "=== Step 6: Retrieving context ===\n",
      "Question: गांधीजी की शादी किससे हुई थी?\n",
      "Context tokens: 258\n",
      "\n",
      "=== Step 7: Building prompt ===\n",
      "Prompt length: 334 tokens\n",
      "\n",
      "=== Step 8: Generating answer ===\n",
      "Raw generated: ⁇\n",
      "\n",
      "Final Answer: ⁇\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "============================================================\n",
      "QUESTION: फ्रांस की राजधानी क्या है?\n",
      "============================================================\n",
      "\n",
      "Loading PDF...\n",
      "Raw text length: 3162 chars\n",
      "Clean text length: 2361 chars\n",
      "Number of chunks: 4\n",
      "Sample chunk: महात्मा गांधी की जीवनी महात्मा गांधी, जिन्हेंबापूया राष्ट्रपिता कहा जाता है, भारतकेस्वतंत्रता संग्रामकेसबसेप्रमुखनेता थे।उनका पूरा नाम मोहनदासकरमचंदगांधी था। जन्मऔरप्रारंभिकजीवन महात्मा गांधी का जन्म2 अक्टूबर1869 को गुजरातकेपोरबंदरमेंहुआथा।उनकेपिता करमचंदगांधी पोरबंदरराज्य केदीवानथेऔरमाता पुतलीबाईधा...\n",
      "\n",
      "=== Step 5: Building FAISS index ===\n",
      "Filtered chunks: 4 / 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e6a52017344fd1bbfc58660dd3d79d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built!\n",
      "\n",
      "=== Step 6: Retrieving context ===\n",
      "Question: फ्रांस की राजधानी क्या है?\n",
      "Context tokens: 262\n",
      "\n",
      "=== Step 7: Building prompt ===\n",
      "Prompt length: 337 tokens\n",
      "\n",
      "=== Step 8: Generating answer ===\n",
      "Raw generated: ⁇\n",
      "\n",
      "Final Answer: ⁇\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run example\n",
    "pdf_file = \"gandhi.pdf\"  # change to your PDF\n",
    "questions = [\n",
    "    \"महात्मा गांधी का जन्म कब हुआ था?\",\n",
    "    \"उनके पिता का नाम क्या था?\",\n",
    "    \"गांधीजी की शादी किससे हुई थी?\",\n",
    "    \"फ्रांस की राजधानी क्या है?\"  # fallback check\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    rag_on_pdf(pdf_file, q)\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3de9a2-dc42-469b-b9b1-75e5ea6dffb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
