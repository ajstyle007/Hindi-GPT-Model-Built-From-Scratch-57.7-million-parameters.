{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46dc1887-b215-444e-a766-20ebc875a867",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "def load_pdf_text(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        if page.extract_text():\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def chunk_text_tokens(text, chunk_size=256, overlap=64):\n",
    "    ids = sp.encode(text)\n",
    "    chunks = []\n",
    "\n",
    "    start = 0\n",
    "    while start < len(ids):\n",
    "        end = start + chunk_size\n",
    "        chunk_ids = ids[start:end]\n",
    "        chunks.append(chunk_ids)\n",
    "        start = end - overlap\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "888c7105-6fa0-459b-ae90-057fe247cb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embed_model = SentenceTransformer(\n",
    "    \"intfloat/multilingual-e5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6df26617-5865-4b25-a3d2-1602d5b83285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4201e52112c3429bb5852573718fa132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/891 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kumar\\.cache\\huggingface\\hub\\models--cross-encoder--mmarco-mMiniLMv2-L12-H384-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1279ae870454844ae94927610e54051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6c668cdbb54658b761ef633d5cbdaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/435 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "871b004a5d9e4906a7a69b8d2b366528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeedbdc6b7904774b3914726842d86c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cae94e5c6894bc0bc4ec32a42f5102c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "reranker = CrossEncoder(\n",
    "    \"cross-encoder/mmarco-mMiniLMv2-L12-H384-v1\"\n",
    ")\n",
    "\n",
    "def rerank(query, candidate_texts, top_k=3):\n",
    "    pairs = [(query, t) for t in candidate_texts]\n",
    "    scores = reranker.predict(pairs)\n",
    "\n",
    "    ranked = sorted(\n",
    "        zip(candidate_texts, scores),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )\n",
    "    return [x[0] for x in ranked[:top_k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e4e1c1c-d62f-4407-b9f7-509dbb5531d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "def build_faiss_index(chunk_token_ids):\n",
    "    chunk_texts = [sp.decode(ids) for ids in chunk_token_ids]\n",
    "\n",
    "    embeddings = embed_model.encode(\n",
    "        [\"passage: \" + t for t in chunk_texts],\n",
    "        normalize_embeddings=True,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "\n",
    "    return index, chunk_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "60aa2d8d-dd25-4812-977b-e5261e07532d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context_tokens(query, chunks, index, top_k=8):\n",
    "    query_emb = embed_model.encode(\n",
    "        [\"query: \" + query],\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "\n",
    "    scores, idxs = index.search(query_emb, top_k)\n",
    "\n",
    "    context_ids = []\n",
    "    for score, i in zip(scores[0], idxs[0]):\n",
    "        chunk_text = chunks[i]\n",
    "\n",
    "        # üî• HARD KEYWORD FILTER\n",
    "        if not (\"‡§ß‡§®‡§ø‡§Ø‡§æ\" in chunk_text or \"‡§π‡•ã‡§∞‡•Ä\" in chunk_text):\n",
    "            continue\n",
    "\n",
    "        if score < 0.4:\n",
    "            continue\n",
    "\n",
    "        context_ids += sp.encode(\n",
    "            f\"\\n[‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠]\\n{chunk_text}\\n\"\n",
    "        )\n",
    "\n",
    "    return context_ids[:1200]\n",
    "\n",
    "\n",
    "\n",
    "# def build_prompt(context, question):\n",
    "#     prompt = f\"\"\"\n",
    "#     ‡§®‡•Ä‡§ö‡•á ‡§¶‡§ø‡§è ‡§ó‡§è ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§ï‡•á ‡§Ü‡§ß‡§æ‡§∞ ‡§™‡§∞ ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§ï‡§æ\n",
    "#     ‡§∏‡§ø‡§∞‡•ç‡§´‡§º ‡§∏‡•ç‡§™‡§∑‡•ç‡§ü ‡§î‡§∞ ‡§∏‡§Ç‡§ï‡•ç‡§∑‡§ø‡§™‡•ç‡§§ ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•Ä‡§ú‡§ø‡§è‡•§\n",
    "#     ‡§Ö‡§ó‡§∞ ‡§â‡§§‡•ç‡§§‡§∞ ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§Æ‡•á‡§Ç ‡§® ‡§π‡•ã, ‡§§‡•ã \"‡§Æ‡•Å‡§ù‡•á ‡§®‡§π‡•Ä‡§Ç ‡§™‡§§‡§æ\" ‡§≤‡§ø‡§ñ‡§ø‡§è‡•§\n",
    "\n",
    "#     ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠:\n",
    "#     {context}\n",
    "\n",
    "#     ‡§™‡•ç‡§∞‡§∂‡•ç‡§®:\n",
    "#     {question}\n",
    "\n",
    "#     ‡§â‡§§‡•ç‡§§‡§∞:\n",
    "#     \"\"\"\n",
    "#     return prompt.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75857669-a446-4aa8-ab98-82fa7ec5dee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "# Load tokenizer\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"hindi_tokenizer_new.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72776c6a-ee68-44f2-92ee-b5627341874b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from decoder_only_gpt import My_GPT_model\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d8b6281-546c-44f5-82eb-9a5429206bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "My_GPT_model(\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(32768, 512)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x Decoder_GPT_Block(\n",
       "        (swi_glu): SwiGLU_FFN(\n",
       "          (w1): Linear(in_features=512, out_features=1536, bias=False)\n",
       "          (w2): Linear(in_features=512, out_features=1536, bias=False)\n",
       "          (w3): Linear(in_features=1536, out_features=512, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (masked_mha): Masked_MHA(\n",
       "          (Q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (K): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (V): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (rms_norm0): RMSNorm()\n",
       "        (rms_norm1): RMSNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = My_GPT_model(vocab_size=sp.get_piece_size(),num_layers=12,\n",
    "        d_model=512,d_ff=2048,num_heads=8,seq_len=512).to(DEVICE)\n",
    "# model = torch.compile(model)   # üî• IMPORTANT\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "382ae2b6-5990-45b0-baa5-9b62c385e455",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(\"checkpoints_HindiGPT-v1_step280000.pt\", map_location=DEVICE)\n",
    "state_dict = ckpt[\"model\"]\n",
    "\n",
    "clean_state_dict = {}\n",
    "\n",
    "for k, v in state_dict.items():\n",
    "    if k.startswith(\"_orig_mod.\"):\n",
    "        clean_state_dict[k.replace(\"_orig_mod.\", \"\")] = v\n",
    "    else:\n",
    "        clean_state_dict[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08793f07-b919-42a8-b088-26a45dd1de1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "My_GPT_model(\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(32768, 512)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x Decoder_GPT_Block(\n",
       "        (swi_glu): SwiGLU_FFN(\n",
       "          (w1): Linear(in_features=512, out_features=1536, bias=False)\n",
       "          (w2): Linear(in_features=512, out_features=1536, bias=False)\n",
       "          (w3): Linear(in_features=1536, out_features=512, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (masked_mha): Masked_MHA(\n",
       "          (Q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (K): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (V): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (rms_norm0): RMSNorm()\n",
       "        (rms_norm1): RMSNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing, unexpected = model.load_state_dict(clean_state_dict, strict=False)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4afed046-0928-4a76-b709-6ed6d0bd3435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "# def clean_hindi_text(text: str) -> str:\n",
    "#     # Unicode NFC normalize\n",
    "#     text = unicodedata.normalize(\"NFC\", text)\n",
    "\n",
    "#     # remove space between base + matra\n",
    "#     text = re.sub(r\"([‡§ï-‡§π])\\s+([‡§æ‡§ø‡•Ä‡•Å‡•Ç‡•á‡•à‡•ã‡•å‡§Ç‡§É‡§Å])\", r\"\\1\\2\", text)\n",
    "\n",
    "#     # collapse repeated characters\n",
    "#     text = re.sub(r\"(.)\\1{5,}\", r\"\\1\", text)\n",
    "\n",
    "#     return text\n",
    "def clean_hindi_text(text):\n",
    "    text = re.sub(r\"[‚ÅáÔøΩ]\", \"\", text)       # OCR garbage\n",
    "    text = re.sub(r\"\\s+\", \" \", text)       # extra spaces\n",
    "    text = text.replace(\" ‡•ã\", \"‡•ã\")\n",
    "    text = text.replace(\" ‡•à\", \"‡•à\")\n",
    "    text = text.replace(\" ‡•Ä\", \"‡•Ä\")\n",
    "    text = text.replace(\" ‡•Å\", \"‡•Å\")\n",
    "    text = text.replace(\" ‡•Ç\", \"‡•Ç\")\n",
    "    text = re.sub(r\"[^\\u0900-\\u097F‡•§\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f00e446-2c4a-458a-af8a-9435a884a582",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_ID = sp.eos_id()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_answer_from_ids(prompt_ids, max_new_tokens=400, temperature=0.3, top_p=0.9, repetition_penalty=1.15, penalty_window=128):\n",
    "    model.eval()\n",
    "    MAX_SEQ_LEN = 512\n",
    "\n",
    "    # ---------- GUARANTEED input_ids ----------\n",
    "    if isinstance(prompt_ids, torch.Tensor):\n",
    "        input_ids = prompt_ids.clone().detach()\n",
    "    elif isinstance(prompt_ids, list):\n",
    "        input_ids = torch.tensor(prompt_ids, dtype=torch.long)\n",
    "    else:\n",
    "        raise TypeError(f\"prompt_ids must be list or tensor, got {type(prompt_ids)}\")\n",
    "\n",
    "    # shape fix\n",
    "    if input_ids.dim() == 1:\n",
    "        input_ids = input_ids.unsqueeze(0)\n",
    "\n",
    "    input_ids = input_ids.to(DEVICE)\n",
    "\n",
    "    # hard truncate\n",
    "    if input_ids.shape[1] > MAX_SEQ_LEN:\n",
    "        input_ids = input_ids[:, -MAX_SEQ_LEN:]\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(input_ids)                     # (B, S, vocab)\n",
    "        next_token_logits = logits[:, -1, :]          # (B, vocab)\n",
    "\n",
    "        # -------- Apply repetition penalty --------\n",
    "        if repetition_penalty != 1.0:\n",
    "            recent_tokens = input_ids[0, -penalty_window:].tolist()\n",
    "            for token_id in set(recent_tokens):\n",
    "                next_token_logits[0, token_id] /= repetition_penalty\n",
    "\n",
    "        # -------- Apply temperature --------\n",
    "        next_token_logits = next_token_logits / temperature\n",
    "\n",
    "        # -------- Top-p (nucleus) filtering --------\n",
    "        probs = torch.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above top_p\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift right to keep at least one token\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = False\n",
    "\n",
    "        sorted_probs[sorted_indices_to_remove] = 0\n",
    "        sorted_probs /= sorted_probs.sum(dim=-1, keepdim=True)\n",
    "\n",
    "        # Sample next token from filtered distribution\n",
    "        next_token = torch.multinomial(sorted_probs, num_samples=1)\n",
    "        next_token = sorted_indices.gather(-1, next_token)\n",
    "\n",
    "        # Append next token\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "\n",
    "        # Sliding window\n",
    "        if input_ids.shape[1] > MAX_SEQ_LEN:\n",
    "            input_ids = input_ids[:, -MAX_SEQ_LEN:]\n",
    "\n",
    "        # Stop at EOS\n",
    "        if EOS_ID != -1 and next_token.item() == EOS_ID:\n",
    "            break\n",
    "\n",
    "    return sp.decode(input_ids[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b58acef2-44d7-46ef-8215-16bd5baedf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def sentence_chunk_hindi(text, max_tokens=256, overlap=2):\n",
    "    sentences = re.split(r\"(‡•§|\\n)\", text)\n",
    "    sentences = [\"\".join(sentences[i:i+2]) for i in range(0, len(sentences), 2)]\n",
    "\n",
    "    chunks = []\n",
    "    current = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        current.append(sent)\n",
    "        token_len = len(sp.encode(\" \".join(current)))\n",
    "\n",
    "        if token_len >= max_tokens:\n",
    "            chunks.append(\" \".join(current))\n",
    "            current = current[-overlap:]\n",
    "\n",
    "    if current:\n",
    "        chunks.append(\" \".join(current))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "53434bc5-3f3a-4ced-9749-262856019562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_ids(context_ids, question):\n",
    "    instruction = \"\"\"\n",
    "‡§Ü‡§™ ‡§™‡•ç‡§∞‡•á‡§Æ‡§ö‡§Ç‡§¶ ‡§ï‡•á ‡§â‡§™‡§®‡•ç‡§Ø‡§æ‡§∏ '‡§ó‡•ã‡§¶‡§æ‡§®' ‡§ï‡•á ‡§µ‡§ø‡§∂‡•á‡§∑‡§ú‡•ç‡§û ‡§π‡•à‡§Ç‡•§\n",
    "‡§®‡•Ä‡§ö‡•á ‡§¶‡§ø‡§è ‡§ó‡§è ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§ï‡•á ‡§Ü‡§ß‡§æ‡§∞ ‡§™‡§∞ ‡§π‡•Ä ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•Ä‡§ú‡§ø‡§è‡•§\n",
    "\n",
    "‚ö†Ô∏è ‡§Ø‡§¶‡§ø ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§ß‡§®‡§ø‡§Ø‡§æ ‡§∏‡•á ‡§∏‡§Ç‡§¨‡§Ç‡§ß‡§ø‡§§ ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à,\n",
    "‡§§‡•ã ‡§ï‡•á‡§µ‡§≤ ‡§Ø‡§π ‡§≤‡§ø‡§ñ‡§ø‡§è:\n",
    "\"‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§Æ‡•á‡§Ç ‡§ß‡§®‡§ø‡§Ø‡§æ ‡§∏‡•á ‡§∏‡§Ç‡§¨‡§Ç‡§ß‡§ø‡§§ ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à‡•§\"\n",
    "\n",
    "‡§ï‡§ø‡§∏‡•Ä ‡§≠‡•Ä ‡§∏‡•ç‡§•‡§ø‡§§‡§ø ‡§Æ‡•á‡§Ç ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§ï‡•ã ‡§¶‡•ã‡§π‡§∞‡§æ‡§è‡§Å ‡§®‡§π‡•Ä‡§Ç‡•§\n",
    "\"\"\"\n",
    "\n",
    "    return (\n",
    "        [sp.bos_id()]\n",
    "        + sp.encode(instruction)\n",
    "        + sp.encode(\"\\n\\n‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠:\\n\")\n",
    "        + context_ids\n",
    "        + sp.encode(\"\\n\\n‡§™‡•ç‡§∞‡§∂‡•ç‡§®:\\n\")\n",
    "        + sp.encode(question)\n",
    "        + sp.encode(\"\\n\\n‡§â‡§§‡•ç‡§§‡§∞:\\n\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60b99d69-74ed-40b8-b4c2-7fe26b574280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def load_pdf_text_pymupdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2248ed09-0a8a-460a-8301-957e19fa2357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_faiss_index_text(chunks):\n",
    "    embeddings = embed_model.encode(\n",
    "        [\"passage: \" + t for t in chunks],\n",
    "        normalize_embeddings=True,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab63a538-21f1-4029-849c-d770eabc0619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47925b095c434d5b904f35672b9e5cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load PDF\n",
    "pdf_text = load_pdf_text(\"godan_by_premchand.pdf\")\n",
    "pdf_text = clean_hindi_text(pdf_text)\n",
    "\n",
    "# Chunk\n",
    "chunks = sentence_chunk_hindi(pdf_text)\n",
    "\n",
    "# Build index\n",
    "index = build_faiss_index_text(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "327a518f-3da6-4b17-a30f-118a9234b25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚Åá ‡§Ü‡§™ ‡§™‡•ç‡§∞‡•á‡§Æ‡§ö‡§Ç‡§¶ ‡§ï‡•á ‡§â‡§™‡§®‡•ç‡§Ø‡§æ‡§∏ '‡§ó‡•ã‡§¶‡§æ‡§®' ‡§ï‡•á ‡§µ‡§ø‡§∂‡•á‡§∑‡§ú‡•ç‡§û ‡§π‡•à‡§Ç‡•§ ‚Åá ‡§®‡•Ä‡§ö‡•á ‡§¶‡§ø‡§è ‡§ó‡§è ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§ï‡•á ‡§Ü‡§ß‡§æ‡§∞ ‡§™‡§∞ ‡§π‡•Ä ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•Ä‡§ú‡§ø‡§è‡•§ ‚Åá  ‡§Ø‡§¶‡§ø ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§ß‡§®‡§ø‡§Ø‡§æ ‡§∏‡•á ‡§∏‡§Ç‡§¨‡§Ç‡§ß‡§ø‡§§ ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à, ‚Åá ‡§§‡•ã ‡§ï‡•á‡§µ‡§≤ ‡§Ø‡§π ‡§≤‡§ø‡§ñ‡§ø‡§è: ‚Åá \"‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§Æ‡•á‡§Ç ‡§ß‡§®‡§ø‡§Ø‡§æ ‡§∏‡•á ‡§∏‡§Ç‡§¨‡§Ç‡§ß‡§ø‡§§ ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à‡•§\" ‚Åá ‡§ï‡§ø‡§∏‡•Ä ‡§≠‡•Ä ‡§∏‡•ç‡§•‡§ø‡§§‡§ø ‡§Æ‡•á‡§Ç ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§ï‡•ã ‡§¶‡•ã‡§π‡§∞‡§æ‡§è‡§Å ‡§®‡§π‡•Ä‡§Ç‡•§ ‚Åá   ‚Åá ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠: ‚Åá   ‚Åá ‡§™‡•ç‡§∞‡§∂‡•ç‡§®: ‚Åá   ‚Åá 30 ‡§Æ‡§æ‡§∞‡•ç‡§ö 2015 ‡§ï‡•ã, ‡§Ø‡§π ‡§ò‡•ã‡§∑‡§£‡§æ ‡§ï‡•Ä ‡§ó‡§à ‡§ï‡§ø ‡§¨‡•á‡§Ø‡•ã‡§Ç‡§∏ ‡§è‡§ï ‡§∏‡§π-‡§∏‡•ç‡§µ‡§æ‡§Æ‡•Ä ‡§π‡•à, ‡§µ‡§ø‡§≠‡§ø‡§®‡•ç‡§® ‡§Ö‡§®‡•ç‡§Ø ‡§∏‡§Ç‡§ó‡•Ä‡§§ ‡§ï‡§≤‡§æ‡§ï‡§æ‡§∞‡•ã‡§Ç ‡§ï‡•á ‡§∏‡§æ‡§•, ‡§∏‡§Ç‡§ó‡•Ä‡§§ ‡§∏‡•ç‡§ü‡•ç‡§∞‡•Ä‡§Æ‡§ø‡§Ç‡§ó ‡§∏‡•á‡§µ‡§æ ‡§ú‡•ç‡§µ‡§æ‡§∞‡•Ä‡§Ø ‡§Æ‡•á‡§Ç ‡•§ ‡§Ø‡§π ‡§∏‡•á‡§µ‡§æ ‡§π‡§æ‡§®‡§ø‡§∞‡§π‡§ø‡§§ ‡§ë‡§°‡§ø‡§Ø‡•ã ‡§î‡§∞ ‡§â‡§ö‡•ç‡§ö ‡§™‡§∞‡§ø‡§≠‡§æ‡§∑‡§æ ‡§∏‡§Ç‡§ó‡•Ä‡§§ ‡§µ‡•Ä‡§°‡§ø‡§Ø‡•ã ‡§Æ‡•á‡§Ç ‡§Æ‡§æ‡§π‡§ø‡§∞ ‡§π‡•à ‡•§ ‡§¨‡•á‡§Ø‡•ã‡§Ç‡§∏ ‡§ï‡•á ‡§™‡§§‡§ø ‡§ú‡•á ‡§ú‡•á‡§° ‡§®‡•á ‡§ú‡•ç‡§µ‡§æ‡§∞‡•Ä‡§Ø ‡§î‡§∞ ‡§ú‡•á-‡§ú‡•á‡§°, ‡§∏‡•ã‡§≤‡§π ‡§ï‡§≤‡§æ‡§ï‡§æ‡§∞ ‡§π‡§ø‡§§‡§ß‡§æ‡§∞‡§ï‡•ã‡§Ç  ‚Åá ‡§ú‡•à‡§∏‡•á ‡§ï‡§æ‡§®‡•ç‡§Ø‡•á ‡§µ‡•á‡§∏‡•ç‡§ü, ‡§∞‡§ø‡§π‡§æ‡§®‡§æ, ‡§Æ‡•à‡§°‡•ã‡§®‡§æ, ‡§ï‡•ç‡§∞‡§ø‡§∏ ‡§Æ‡§æ‡§∞‡•ç‡§ü‡§ø‡§®, ‡§®‡§ø‡§ï‡•Ä ‡§Æ‡§ø‡§®‡§æ‡§ú ‡§î‡§∞ ‡§Ö‡§ß‡§ø‡§ï ‚Åá  ‡§∏‡§π‡§ø‡§§ 2015. ‡§ï‡•Ä ‡§™‡§π‡§≤‡•Ä ‡§§‡§ø‡§Æ‡§æ‡§π‡•Ä ‡§Æ‡•á‡§Ç ‡§ú‡•ç‡§µ‡§æ‡§∞‡•Ä‡§Ø, aspiro ‡§ï‡•Ä ‡§ú‡§®‡§ï ‡§ï‡§Ç‡§™‡§®‡•Ä ‡§ï‡§æ ‡§Ö‡§ß‡§ø‡§ó‡•ç‡§∞‡§π‡§£ ‡§ï‡§ø‡§Ø‡§æ ‡•§ ‡§∏‡§π-‡§∏‡•ç‡§µ‡§Ø‡§Ç ‡§ú‡•ç‡§µ‡§æ‡§∞‡•Ä‡§Ø, ‡§¨‡§π‡•Å‡§Æ‡§§ ‡§ï‡•á ‡§∏‡§æ‡§• 3  ‚Åá  ‡§á‡§ï‡•ç‡§µ‡§ø‡§ü‡•Ä ‡§π‡§ø‡§∏‡•ç‡§∏‡•á‡§¶‡§æ‡§∞‡•Ä ‡§π‡•à ‡•§ ‡§è‡§ï ‡§∏‡§≠‡•Ä ‡§ï‡§≤‡§æ‡§ï‡§æ‡§∞‡•ã‡§Ç ‡§ï‡•á ‡§∏‡•ç‡§µ‡§æ‡§Æ‡§ø‡§§‡•ç‡§µ ‡§µ‡§æ‡§≤‡•Ä ‡§∏‡•ç‡§ü‡•ç‡§∞‡•Ä‡§Æ‡§ø‡§Ç‡§ó ‡§∏‡•á‡§µ‡§æ ‡§π‡•ã‡§®‡•á ‡§ï‡§æ ‡§µ‡§ø‡§ö‡§æ‡§∞ ‡§â‡§® ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§¨‡§®‡§æ‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§•‡§æ ‡§ú‡•ã ‡§µ‡§∞‡•ç‡§§‡§Æ‡§æ‡§® ‡§∏‡§Ç‡§ó‡•Ä‡§§ ‡§â‡§¶‡•ç‡§Ø‡•ã‡§ó ‡§ï‡•á ‡§≠‡•Ä‡§§‡§∞ ‡§∏‡•ç‡§ü‡•ç‡§∞‡•Ä‡§Æ‡§ø‡§Ç‡§ó ‡§ï‡•Ä ‡§¨‡§¢‡§º‡•Ä ‡§Æ‡§æ‡§Ç‡§ó ‡§ï‡•ã ‡§Ö‡§®‡•Å‡§ï‡•Ç‡§≤‡§ø‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§∂‡§æ‡§Æ‡§ø‡§≤ ‡§•‡•á, ‡§î‡§∞ spotify ‡§ú‡•à‡§∏‡•á ‡§Ö‡§®‡•ç‡§Ø ‡§∏‡•ç‡§ü‡•ç‡§∞‡•Ä‡§Æ‡§ø‡§Ç‡§ó ‡§∏‡•á‡§µ‡§æ‡§ì‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è, ‡§ú‡•ã ‡§â‡§®‡§ï‡•á ‡§ï‡§Æ ‡§≠‡•Å‡§ó‡§§‡§æ‡§® ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ü‡§≤‡•ã‡§ö‡§®‡§æ ‡§ï‡•Ä ‡§ó‡§à ‡§π‡•à ‡§∞‡•â‡§Ø‡§≤‡•ç‡§ü‡•Ä ‡§ï‡§æ ‡•§ ‡§ö‡•Å‡§®‡•å‡§§‡•Ä ‡§π‡•à ‡§ï‡§ø ‡§π‡§∞ ‡§ï‡§ø‡§∏‡•Ä ‡§ï‡•ã ‡§´‡§ø‡§∞ ‡§∏‡•á ‡§∏‡§Ç‡§ó‡•Ä‡§§ ‡§ï‡§æ ‡§∏‡§Æ‡•ç‡§Æ‡§æ‡§® ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è, ‡§Ö‡§™‡§®‡•á ‡§Æ‡•Ç‡§≤‡•ç‡§Ø ‡§ï‡•ã ‡§™‡§π‡§ö‡§æ‡§®‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è, ‡§ú‡•ç‡§µ‡§æ‡§∞‡•Ä‡§Ø ‡§ï‡•Ä ‡§∞‡§ø‡§π‡§æ‡§à ‡§™‡§∞ ‡§ú‡•á-‡§ú‡•á‡§° ‡§ï‡§π‡§æ ‡§ó‡§Ø‡§æ ‡•§ ‚Åá  ‡§ú‡•ç‡§µ‡§æ‡§∞‡•Ä‡§Ø ‡§ï‡•Ä ‡§ú‡§®‡§ï ‡§ï‡§Ç‡§™‡§®‡•Ä 2015 ‡§Æ‡•á‡§Ç ‡§ï‡§ø‡§∏‡§ï‡•á ‡§∏‡•ç‡§µ‡§æ‡§Æ‡§ø‡§§‡•ç‡§µ ‡§Æ‡•á‡§Ç ‡§¨‡§®‡•Ä? ‚Åá   ‚Åá ‡§â‡§§‡•ç‡§§‡§∞: ‚Åá ‡•ç‡§∏‡§®‡•ç‡§§‡§ø ‡§®‡•á ‡§Ö‡§™‡§®‡•Ä ‡§µ‡•á‡§¨‡§∏‡§æ‡§á‡§ü ‡§™‡§∞ ‡§≤‡§ø‡§ñ‡§æ ‡§π‡•à ‡§ï‡§ø \"‡§π‡§Æ ‡§Ö‡§™‡§®‡•á ‡§ó‡•ç‡§∞‡§æ‡§π‡§ï‡•ã‡§Ç ‡§ï‡•ã ‡§è‡§ï ‡§∏‡§æ‡§• ‡§≤‡§æ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§™‡•ç‡§∞‡§§‡§ø‡§¨‡§¶‡•ç‡§ß ‡§π‡•à‡§Ç‡•§ ‡§π‡§Æ ‡§Ö‡§™‡§®‡•á ‡§ó‡•ç‡§∞‡§æ‡§π‡§ï‡•ã‡§Ç ‡§ï‡•ã ‡§è‡§ï ‡§∏‡§æ‡§• ‡§≤‡§æ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§™‡•ç‡§∞‡§§‡§ø‡§¨‡§¶‡•ç‡§ß ‡§π‡•à‡§Ç‡•§\"\n"
     ]
    }
   ],
   "source": [
    "# Question\n",
    "question = \"\"\"\n",
    "‡§ß‡§®‡§ø‡§Ø‡§æ ‡§ï‡•á ‡§ö‡§∞‡§ø‡§§‡•ç‡§∞ ‡§ï‡§æ ‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§ ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£ ‡§ï‡•Ä‡§ú‡§ø‡§è‡•§\n",
    "‡§Ø‡§π ‡§∏‡•ç‡§™‡§∑‡•ç‡§ü ‡§ï‡§∞‡•á‡§Ç ‡§ï‡§ø ‡§µ‡§π ‡§π‡•ã‡§∞‡•Ä ‡§∏‡•á ‡§ï‡§ø‡§∏ ‡§§‡§∞‡§π ‡§Ö‡§≤‡§ó ‡§∏‡•ã‡§ö ‡§∞‡§ñ‡§§‡•Ä ‡§π‡•à ‡§î‡§∞ ‡§ï‡•à‡§∏‡•á ‡§â‡§∏‡§ï‡§æ ‡§ö‡§∞‡§ø‡§§‡•ç‡§∞ ‡§ó‡•ç‡§∞‡§æ‡§Æ‡•Ä‡§£ ‡§∏‡§Æ‡§æ‡§ú ‡§Æ‡•á‡§Ç ‡§∏‡•ç‡§§‡•ç‡§∞‡•Ä ‡§ï‡•Ä ‡§Ö‡§∏‡§≤‡•Ä ‡§§‡§æ‡§ï‡§§ ‡§ï‡•ã ‡§¶‡§∞‡•ç‡§∂‡§æ‡§§‡§æ ‡§π‡•à‡•§\n",
    "\"\"\"\n",
    "\n",
    "# Retrieve\n",
    "context_ids = retrieve_context_tokens(question, chunks, index)\n",
    "\n",
    "# Prompt\n",
    "prompt_ids = build_prompt_ids(context_ids, question)\n",
    "\n",
    "# Generate\n",
    "answer = generate_answer_from_ids(prompt_ids)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4e606f3-e9b7-4642-8278-cd8f637596a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PS D:\\deep learning\\research_paper\\Transformers\\decoder_project> python .\\generate.py                                                     \n",
    "# Loading checkpoint...\n",
    "# Initializing model...\n",
    "# Detected torch.compile prefix, stripping '_orig_mod.'...\n",
    "# Model loaded successfully!\n",
    "# Prompt: \n",
    "\n",
    "#     30 ‡§Æ‡§æ‡§∞‡•ç‡§ö 2015 ‡§ï‡•ã, ‡§Ø‡§π ‡§ò‡•ã‡§∑‡§£‡§æ ‡§ï‡•Ä ‡§ó‡§à ‡§ï‡§ø ‡§¨‡•á‡§Ø‡•ã‡§Ç‡§∏ ‡§è‡§ï ‡§∏‡§π-‡§∏‡•ç‡§µ‡§æ‡§Æ‡•Ä ‡§π‡•à, ‡§µ‡§ø‡§≠‡§ø‡§®‡•ç‡§® ‡§Ö‡§®‡•ç‡§Ø ‡§∏‡§Ç‡§ó‡•Ä‡§§ ‡§ï‡§≤‡§æ‡§ï‡§æ‡§∞‡•ã‡§Ç ‡§ï‡•á ‡§∏‡§æ‡§•, ‡§∏‡§Ç‡§ó‡•Ä‡§§ ‡§∏‡•ç‡§ü‡•ç‡§∞‡•Ä‡§Æ‡§ø‡§Ç‡§ó ‡§∏‡•á‡§µ‡§æ ‡§ú‡•ç‡§µ‡§æ‡§∞‡•Ä‡§Ø ‡§Æ‡•á‡§Ç ‡•§ ‡§Ø‡§π ‡§∏‡•á‡§µ‡§æ ‡§π‡§æ‡§®‡§ø‡§∞‡§π‡§ø‡§§ ‡§ë‡§°‡§ø‡§Ø‡•ã ‡§î‡§∞ ‡§â‡§ö‡•ç‡§ö ‡§™‡§∞‡§ø‡§≠‡§æ‡§∑‡§æ ‡§∏‡§Ç‡§ó‡•Ä‡§§ ‡§µ‡•Ä‡§°‡§ø‡§Ø‡•ã ‡§Æ‡•á‡§Ç ‡§Æ‡§æ‡§π‡§ø‡§∞ ‡§π‡•à ‡•§ ‡§¨‡•á‡§Ø‡•ã‡§Ç‡§∏ ‡§ï‡•á ‡§™‡§§‡§ø ‡§ú‡•á ‡§ú‡•á‡§° ‡§®‡•á ‡§ú‡•ç‡§µ‡§æ‡§∞‡•Ä‡§Ø ‡§î‡§∞ ‡§ú‡•á-‡§ú‡•á‡§°, ‡§∏‡•ã‡§≤‡§π ‡§ï‡§≤‡§æ‡§ï‡§æ‡§∞ ‡§π‡§ø‡§§‡§ß‡§æ‡§∞‡§ï‡•ã‡§Ç (‡§ú‡•à‡§∏‡•á ‡§ï‡§æ‡§®‡•ç‡§Ø‡•á ‡§µ‡•á‡§∏‡•ç‡§ü, ‡§∞‡§ø‡§π‡§æ‡§®‡§æ, ‡§Æ‡•à‡§°‡•ã‡§®‡§æ, ‡§ï‡•ç‡§∞‡§ø‡§∏ ‡§Æ‡§æ‡§∞‡•ç‡§ü‡§ø‡§®, ‡§®‡§ø‡§ï‡•Ä ‡§Æ‡§ø‡§®‡§æ‡§ú ‡§î‡§∞ ‡§Ö‡§ß‡§ø‡§ï) ‡§∏‡§π‡§ø‡§§ 2015. ‡§ï‡•Ä ‡§™‡§π‡§≤‡•Ä ‡§§‡§ø‡§Æ‡§æ‡§π‡•Ä ‡§Æ‡•á‡§Ç ‡§ú‡•ç‡§µ‡§æ‡§∞‡•Ä‡§Ø, aspiro ‡§ï‡•Ä ‡§ú‡§®‡§ï ‡§ï‡§Ç‡§™‡§®‡•Ä ‡§ï‡§æ ‡§Ö‡§ß‡§ø‡§ó‡•ç‡§∞‡§π‡§£ ‡§ï‡§ø‡§Ø‡§æ ‡•§ ‡§∏‡§π-‡§∏‡•ç‡§µ‡§Ø‡§Ç ‡§ú‡•ç‡§µ‡§æ‡§∞‡•Ä‡§Ø, ‡§¨‡§π‡•Å‡§Æ‡§§ ‡§ï‡•á ‡§∏‡§æ‡§• 3 % ‡§á‡§ï‡•ç‡§µ‡§ø‡§ü‡•Ä ‡§π‡§ø‡§∏‡•ç‡§∏‡•á‡§¶‡§æ‡§∞‡•Ä ‡§π‡•à ‡•§ ‡§è‡§ï ‡§∏‡§≠‡•Ä ‡§ï‡§≤‡§æ‡§ï‡§æ‡§∞‡•ã‡§Ç ‡§ï‡•á ‡§∏‡•ç‡§µ‡§æ‡§Æ‡§ø‡§§‡•ç‡§µ ‡§µ‡§æ‡§≤‡•Ä ‡§∏‡•ç‡§ü‡•ç‡§∞‡•Ä‡§Æ‡§ø‡§Ç‡§ó ‡§∏‡•á‡§µ‡§æ ‡§π‡•ã‡§®‡•á ‡§ï‡§æ ‡§µ‡§ø‡§ö‡§æ‡§∞ ‡§â‡§® ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§¨‡§®‡§æ‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§•‡§æ ‡§ú‡•ã ‡§µ‡§∞‡•ç‡§§‡§Æ‡§æ‡§® ‡§∏‡§Ç‡§ó‡•Ä‡§§ ‡§â‡§¶‡•ç‡§Ø‡•ã‡§ó ‡§ï‡•á ‡§≠‡•Ä‡§§‡§∞ ‡§∏‡•ç‡§ü‡•ç‡§∞‡•Ä‡§Æ‡§ø‡§Ç‡§ó ‡§ï‡•Ä ‡§¨‡§¢‡§º‡•Ä ‡§Æ‡§æ‡§Ç‡§ó ‡§ï‡•ã ‡§Ö‡§®‡•Å‡§ï‡•Ç‡§≤‡§ø‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§∂‡§æ‡§Æ‡§ø‡§≤ ‡§•‡•á, ‡§î‡§∞ spotify ‡§ú‡•à‡§∏‡•á ‡§Ö‡§®‡•ç‡§Ø ‡§∏‡•ç‡§ü‡•ç‡§∞‡•Ä‡§Æ‡§ø‡§Ç‡§ó ‡§∏‡•á‡§µ‡§æ‡§ì‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è, ‡§ú‡•ã ‡§â‡§®‡§ï‡•á ‡§ï‡§Æ ‡§≠‡•Å‡§ó‡§§‡§æ‡§® ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ü‡§≤‡•ã‡§ö‡§®‡§æ ‡§ï‡•Ä ‡§ó‡§à ‡§π‡•à ‡§∞‡•â‡§Ø‡§≤‡•ç‡§ü‡•Ä ‡§ï‡§æ ‡•§ ‡§ö‡•Å‡§®‡•å‡§§‡•Ä ‡§π‡•à ‡§ï‡§ø ‡§π‡§∞ ‡§ï‡§ø‡§∏‡•Ä ‡§ï‡•ã ‡§´‡§ø‡§∞ ‡§∏‡•á ‡§∏‡§Ç‡§ó‡•Ä‡§§ ‡§ï‡§æ ‡§∏‡§Æ‡•ç‡§Æ‡§æ‡§® ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è, ‡§Ö‡§™‡§®‡•á ‡§Æ‡•Ç‡§≤‡•ç‡§Ø ‡§ï‡•ã ‡§™‡§π‡§ö‡§æ‡§®‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è, ‡§ú‡•ç‡§µ‡§æ‡§∞‡•Ä‡§Ø ‡§ï‡•Ä ‡§∞‡§ø‡§π‡§æ‡§à ‡§™‡§∞ ‡§ú‡•á-‡§ú‡•á‡§° ‡§ï‡§π‡§æ ‡§ó‡§Ø‡§æ ‡•§\n",
    "# ‡§ú‡•ç‡§µ‡§æ‡§∞‡•Ä‡§Ø ‡§ï‡•Ä ‡§ú‡§®‡§ï ‡§ï‡§Ç‡§™‡§®‡•Ä 2015 ‡§Æ‡•á‡§Ç ‡§ï‡§ø‡§∏‡§ï‡•á ‡§∏‡•ç‡§µ‡§æ‡§Æ‡§ø‡§§‡•ç‡§µ ‡§Æ‡•á‡§Ç ‡§¨‡§®‡•Ä?\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "# Generating 5 different completions...\n",
    "\n",
    "# --- Generation 1/3 ---\n",
    "# ‡§ú‡•ç‡§µ‡§æ‡§∞‡•Ä‡§Ø ‡§ï‡•Ä ‡§ú‡§®‡§ï ‡§ï‡§Ç‡§™‡§®‡•Ä 2015 ‡§Æ‡•á‡§Ç ‡§ï‡§ø‡§∏‡§ï‡•á ‡§∏‡•ç‡§µ‡§æ‡§Æ‡§ø‡§§‡•ç‡§µ ‡§Æ‡•á‡§Ç ‡§¨‡§®‡•Ä? ‡§ó‡§¨‡§ú‡•ç‡§µ‡§æ‡§∞‡•Ä‡§Ø ‡§ï‡•Ä ‡§ú‡§®‡§ï ‡§ï‡§Ç‡§™‡§®‡•Ä 2013 ‡§Æ‡•á‡§Ç ‡§ï‡§ø‡§∏‡§ï‡•á ‡§∏‡•ç‡§µ‡§æ‡§Æ‡§ø‡§§‡•ç‡§µ ‡§Æ‡•á‡§Ç ‡§¨‡§®‡•Ä? ‡§™‡§¢‡§º‡•á‡§Ç‡§É‡§∏ ‡§ï‡•á ‡§™‡§§‡§ø ‡§ú‡•á ‡§ú‡•á‡§° ‡§®‡•á ‡§∏‡•á‡§µ‡§æ‡§®‡§ø‡§µ‡•É‡§§‡•ç‡§§ ‡§∏‡§Ç‡§ó‡•Ä‡§§‡§ï‡§æ‡§∞ ‡§î‡§∞ ‡§ó‡§æ‡§Ø‡§ï-‡§Ö‡§≠‡§ø‡§®‡•á‡§§‡•ç‡§∞‡•Ä, ‡§ó‡§æ‡§Ø‡§ï-‡§™‡§§‡•ç‡§∞‡§ï‡§æ‡§∞, ‡§ó‡§æ‡§Ø‡§ø‡§ï‡§æ ‡§î‡§∞ ‡§Ö‡§≠‡§ø‡§®‡•á‡§§‡§æ ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á ‡§µ‡§æ‡§™‡§∏‡•Ä ‡§ï‡•Ä ‡•§ ‡§∏‡•ç‡§•‡§æ‡§®‡•Ä‡§Ø ‡§ü‡•á‡§≤‡•Ä‡§µ‡§ø‡§ú‡§® ‡§ö‡•à‡§®‡§≤ ‚Äò‡§¶ ‡§ü‡§æ‡§á‡§Æ‡•ç‡§∏ ‡§®‡§æ‡§â‚Äô ‡§®‡•á ‡§è‡§ï ‡§¨‡§Ø‡§æ                                        ‡§æ‡§® ‡§ú‡§æ‡§∞‡•Ä ‡§ï‡§∞ ‡§ï‡§π‡§æ ‡§ï‡§ø ‡§â‡§®‡§ï‡•Ä ‡§™‡§æ‡§∞‡•ç‡§ü‡•Ä ‡§µ‡§ø‡§ß‡§æ‡§®‡§∏‡§≠‡§æ\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# --- Generation 2/3 ---\n",
    "# ‡§ú‡•ç‡§µ‡§æ‡§∞‡•Ä‡§Ø ‡§ï‡§æ ‡§Æ‡•Ç‡§≤ ‡§®‡§æ‡§Æ ‡§ú‡•á-‡§ú‡•á‡§° ‡§π‡•à ‡•§ ‡§á‡§∏‡§Æ‡•á‡§Ç ‡§∏‡•á, ‡§π‡§Æ ‡§∏‡§¨ ‡§è‡§ï ‡§∏‡§æ‡§• ‡§è‡§ï ‡§∏‡§æ‡§•, ‡§µ‡§ø‡§≠‡§ø‡§®‡•ç‡§® ‡§Ö‡§µ‡§∏‡§∞‡•ã‡§Ç ‡§™‡§∞ ‡§è‡§ï ‡§∏‡§æ‡§• ‡§ï‡§æ‡§Æ ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç‡•§ ‡§á‡§∏ ‡§§‡§∞‡§π ‡§∏‡•á ‡§π‡§Æ ‡§∏‡§≠‡•Ä ‡§∏‡§Æ‡§Ø ‡§î‡§∞ ‡§∏‡§Æ‡§Ø ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ               ‡§Æ‡•á‡§Ç ‡§ö‡§∞‡•ç‡§ö‡§æ ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç ‡•§ ‡§Æ‡•á‡§Ç '‡§∏‡•à‡§ï‡§∞‡•ç‡§∏' ‡§ï‡§Ç‡§™‡§®‡•Ä ‡§ï‡•Ä ‡§∏‡•ç‡§•‡§æ‡§™‡§®‡§æ ‡§π‡•Å‡§à ‡§•‡•Ä. ‡§§‡§¨ ‡§Ø‡§π ‡§ï‡§Ç‡§™‡§®‡•Ä ‡§µ‡§∞‡•ç‡§∑ ‡§Æ‡•á‡§Ç ‡§¨‡§Ç‡§¶ ‡§π‡•ã ‡§ó‡§à ‡§•‡•Ä. ‡§¨‡§æ‡§¶ ‡§ï‡•á ‡§µ‡§∞‡•ç‡§∑‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§á‡§∏ ‡§ï‡§Ç‡§™‡§®‡•Ä ‡§ï‡•ã ‡§¨‡§Ç‡§¶ ‡§ï‡§∞ ‡§¶‡§ø‡§Ø‡§æ \n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# --- Generation 3/3 ---\n",
    "# ‡§ú‡•ç‡§µ‡§æ‡§∞‡•Ä‡§Ø ‡§ï‡•Ä ‡§ú‡§®‡§ï ‡§ï‡§Ç‡§™‡§®‡•Ä 2015 ‡§Æ‡•á‡§Ç ‡§ï‡§ø‡§∏‡§ï‡•á ‡§∏‡•ç‡§µ‡§æ‡§Æ‡§ø‡§§‡•ç‡§µ ‡§Æ‡•á‡§Ç ‡§¨‡§®‡•Ä? ‡§ñ‡•Å‡§¨‡§∏‡•ç‡§µ‡§æ‡§∞‡•Ä‡§Ø ‡§ï‡•Ä ‡§ú‡§®‡§ï ‡§ï‡§Ç‡§™‡§®‡•Ä 2015 ‡§Æ‡•á‡§Ç ‡§ï‡§ø‡§∏‡§ï‡•á ‡§∏‡•ç‡§µ‡§æ‡§Æ‡§ø‡§§‡•ç‡§µ ‡§Æ‡•á ‡§¨‡§®‡•Ä? ‡§ñ‡•Å‡§¨‡§∏‡•ç‡§µ‡§æ‡§∞‡•Ä‡§Ø ‡§®‡•á ‡§ï‡§Ç‡§™‡§®‡•Ä ‡§ï‡•ã ‡§Ö‡§™‡§®                     ‡§®‡•á ‡§¨‡§æ‡§ú‡§æ‡§∞ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§è‡§ï ‡§®‡§Ø‡§æ ‡§¨‡•ç‡§∞‡§æ‡§Ç‡§° ‡§¨‡§®‡§æ‡§Ø‡§æ ‡§î‡§∞ ‡§¨‡§æ‡§¶ ‡§ï‡•á ‡§µ‡§∞‡•ç‡§∑‡•ã‡§Ç ‡§Æ‡•á ‡§π‡§Æ‡§æ‡§∞‡•á ‡§™‡§æ‡§∏ ‡§ï‡§Ç‡§™‡§®‡•Ä ‡§ï‡•ã ‡§è‡§ï ‡§®‡§è ‡§¨‡•ç‡§∞‡§æ‡§Ç‡§° ‡§ï‡•á ‡§∞‡•Ç‡§™‡§Æ‡•á‡§Ç ‡§∏‡•ç‡§•‡§æ‡§™‡§ø‡§§ ‡§ï‡§ø‡§Ø‡§æ‡•§ ‡§≤‡•á‡§ï‡§ø‡§® ‡§ï‡•ã‡§à ‡§≠‡•Ä ‡§≠‡§æ‡§∞‡§§‡•Ä‡§Ø ‡§Æ‡•Ç‡§≤ ‡§ï‡§æ                    ‡§Ü‡§¶‡§Æ‡•Ä ‡§Ö‡§™‡§®‡•á ‡§¨‡•ç‡§∞‡§æ‡§Ç‡§° ‡§ï‡•ã ‡§∏‡•ç‡§•‡§æ‡§™‡§ø‡§§ ‡§ï‡§∞‡§®‡•á ‡§∏‡•á ‡§™‡•Ä‡§õ‡•á ‡§®‡§π‡•Ä‡§Ç ‡§π‡§ü‡§§‡§æ\n",
    "# ------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a30b3d-7697-43d0-b8c0-5531a2e576b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
