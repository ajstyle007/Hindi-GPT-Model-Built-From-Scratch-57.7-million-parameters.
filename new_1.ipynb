{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95fb7189-07af-433b-9068-5ca6ff49a105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rag_with_my_model.py\n",
    "import torch\n",
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "import sentencepiece as spm\n",
    "from decoder_only_gpt import My_GPT_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7f3da0e-dad9-47e9-b210-9a0b471c5c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 1. Load tokenizer & model (tera trained model)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"hindi_tokenizer_new.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da3916b8-26a8-4653-a679-64b657386089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "My_GPT_model(\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(32768, 512)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x Decoder_GPT_Block(\n",
       "        (swi_glu): SwiGLU_FFN(\n",
       "          (w1): Linear(in_features=512, out_features=1536, bias=False)\n",
       "          (w2): Linear(in_features=512, out_features=1536, bias=False)\n",
       "          (w3): Linear(in_features=1536, out_features=512, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (masked_mha): Masked_MHA(\n",
       "          (Q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (K): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (V): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (rms_norm0): RMSNorm()\n",
       "        (rms_norm1): RMSNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = My_GPT_model(\n",
    "    vocab_size=sp.get_piece_size(),\n",
    "    num_layers=12,\n",
    "    d_model=512,\n",
    "    d_ff=2048,\n",
    "    num_heads=8,\n",
    "    seq_len=512\n",
    ").to(DEVICE)\n",
    "\n",
    "# Load final SFT checkpoint\n",
    "model.load_state_dict(torch.load(\"full_sft_final.pt\", map_location=DEVICE))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be9aac68-6e21-4622-8fdd-25ab214c3cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load embedding & reranker\n",
    "embed_model = SentenceTransformer(\"intfloat/multilingual-e5-base\")\n",
    "reranker = CrossEncoder(\"cross-encoder/mmarco-mMiniLMv2-L12-H384-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9faaa557-7e1c-44b8-9984-5e1897984d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size from tokenizer: 32768\n",
      "Model vocab size: 32768\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocab size from tokenizer:\", sp.get_piece_size())\n",
    "print(\"Model vocab size:\", model.lm_head.out_features)  # should match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6336f4-4b5f-421f-9040-1e3176aa242f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "762df215-2e7b-4b7f-9199-ea49a66a8ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hindi_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    # Remove non-printable characters\n",
    "    text = re.sub(r'[\\x00-\\x1F\\x7F]', ' ', text)\n",
    "\n",
    "    # Fix common PDF junk chars\n",
    "    text = re.sub(r'[�•ﬁﬂ–—]', ' ', text)\n",
    "\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "888438e8-337d-4d5f-aaf6-8ae20cc91f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_pdf_text(raw_text):\n",
    "    text = re.sub(r'\\s+', ' ', raw_text)\n",
    "    text = re.sub(r'([ऀ-ॿ])([A-Za-z0-9])', r'\\1 \\2', text)\n",
    "    text = re.sub(r'([a-zA-Z0-9])([ऀ-ॿ])', r'\\1 \\2', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef7b09c9-8ed9-4047-adc5-eb7e987ce22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed + Cleaned text snippet:\n",
      "सपनों का एक छोटा-सा कारवाँ सुबह की पहली किरण जब खिड़की से झाँकती है, तो लगता है जैसे कोई पुरानी दोस्त मुस्कुरा रही हो। वही दोस्त जो बचपन में कहती थी \"बड़ा होकर क्या बनोगे?\" मैंने जवाब दिया था \"सब कुछ!\" आज भी वही जवाब मेरे सीने में धड़कता है। सिर्फ अब थोड़ा संशय भी साथ चलता है, जैसे कोई पुराना जूता जो पैर में फिट तो है, पर थोड़ा काटता भी है। जीवन एक लंबी ट्रेन यात्रा है। कुछ स्टेशन पर चाय की केतली गरम रहती है, कुछ पर सिर्फ ठंडी हवा और खामोशी। फिर भी हर स्टेशन पर उतरने वाले लोग अलग-अलग कहानियाँ छो\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"story.pdf\"\n",
    "doc = fitz.open(pdf_path)\n",
    "\n",
    "raw_text = \"\"\n",
    "for page in doc:\n",
    "    raw_text += page.get_text(\"text\")\n",
    "\n",
    "doc.close()\n",
    "\n",
    "fixed_text = fix_pdf_text(raw_text)\n",
    "cleaned_text = clean_hindi_text(fixed_text)\n",
    "\n",
    "print(f\"Fixed + Cleaned text snippet:\\n{cleaned_text[:500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05ca08ab-2d09-4c60-9b18-2337572cb51a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'सपनों का एक छोटा-सा कारवाँ सुबह की पहली किरण जब खिड़की से झाँकती है, तो लगता है जैसे कोई पुरानी दोस्त मुस्कुरा रही हो। वही दोस्त जो बचपन में कहती थी \"बड़ा होकर क्या बनोगे?\" मैंने जवाब दिया था \"सब कुछ!\" आज भी वही जवाब मेरे सीने में धड़कता है। सिर्फ अब थोड़ा संशय भी साथ चलता है, जैसे कोई पुराना जूता जो पैर में फिट तो है, पर थोड़ा काटता भी है। जीवन एक लंबी ट्रेन यात्रा है। कुछ स्टेशन पर चाय की केतली गरम रहती है, कुछ पर सिर्फ ठंडी हवा और खामोशी। फिर भी हर स्टेशन पर उतरने वाले लोग अलग-अलग कहानियाँ छोड़ जाते हैं। कभी कोई बूढ़ी दादी अपनी पोती को समझाती दिखती है \"बेटा, धैर्य रखो, सब ठीक हो जाएगा।\" कभी कोई नौजवान फोन पर चिल्लाता है \"मैंने कहा ना, अगले महीने पैसा भेज दूँगा!\" और मैं? मैं बस खिड़की से बाहर देखता हूँ। खेतों में हल चलाते बैल, दूर पहाड़ों पर बादल का खेल, और बीच-बीच में उड़ते पंछी जो शायद किसी सपने की ओर जा रहे हैं। कभी-कभी सोचता हूँ सपने सच होते हैं या हम सपनों को सच मान लेते हैं? शायद दोनों। क्योंकि जो इंसान सपना देखना छोड़ देता है, वह जीना भी धीरे-धीरे छोड़ देता है। कल एक बच्चे ने मुझसे पूछा \"अंकल, आप खुश हो?\" मैंने हँसकर कहा \"हाँ बेटा।\" फिर मन ही मन सोचा खुशी कोई स्थायी अवस्था नहीं होती, वह एक छोटी-छोटी पलकों के झपकने के बीच आ-जा जाती है। जैसे सुबह की ओस, जैसे माँ की गोद में सो जाना, जैसे पुराने दोस्त का अचानक मैसेज \"यार कितने दिन हो गए!\" जीवन में बहुत कुछ चाहिए होता है पैसा, नाम, सम्मान, प्यार। पर सबसे ज्यादा जरूरत होती है उस छोटी-सी उम्मीद की, जो कहती है: \"कल बेहतर होगा।\" और अगर कल बेहतर न भी हुआ, तो भी हम कह सकते हैं \"कोशिश तो की थी ना?\" बस यही काफी है। क्योंकि जो कोशिश करता है, वह हारा हुआ नहीं होता। वह बस थोड़ा रुका हुआ होता है। और रुकना भी यात्रा का हिस्सा है। तो चलो, फिर से बैग उठाते हैं। फिर से टिकट चेक करते हैं। फिर से खिड़की के पास वाली सीट ढूँढते हैं। क्योंकि अभी बहुत सारे स्टेशन बाकी हैं। और हर स्टेशन पर कोई न कोई नई कहानी इंतज़ार कर रही है। धन्यवाद जीवन, तुम थोड़े जटिल हो, पर बहुत खूबसूरत भी हो। एक साधारण यात्री'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d76b498c-2e6d-4f07-8edf-528a59604b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chunking function (sentence level)\n",
    "def chunk_text(text, max_tokens=250, overlap=50):\n",
    "    tokens = sp.encode(text)\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        chunk = tokens[i:i + max_tokens]\n",
    "        chunks.append(sp.decode(chunk))\n",
    "        i += max_tokens - overlap\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f215d1e-f4f9-4a04-b249-afe58948c10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_chunk_text(text, max_chars=800, overlap_sentences=1):\n",
    "    sentences = re.split(r'(?<=[।!?])\\s+', text)\n",
    "\n",
    "    chunks = []\n",
    "    current_sents = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        current_sents.append(sent)\n",
    "\n",
    "        chunk_text = \" \".join(current_sents)\n",
    "\n",
    "        if len(chunk_text) >= max_chars:\n",
    "            chunks.append(chunk_text.strip())\n",
    "\n",
    "            # overlap: last N full sentences\n",
    "            current_sents = current_sents[-overlap_sentences:]\n",
    "\n",
    "    if current_sents:\n",
    "        chunks.append(\" \".join(current_sents).strip())\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e2c1fde-610f-4b34-a241-7c36c23e7226",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build FAISS index\n",
    "def build_index(chunks):\n",
    "    texts = [\"passage: \" + chunk for chunk in chunks]\n",
    "    embeddings = embed_model.encode(texts, normalize_embeddings=True, batch_size=32)\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(embeddings.astype(np.float32))\n",
    "    return index, chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "62a7a07e-1a41-41f0-b42c-8152f6ec6ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(\n",
    "    model,\n",
    "    input_ids,\n",
    "    max_new_tokens=120,\n",
    "    temperature=0.75,\n",
    "    top_p=0.92,\n",
    "    repetition_penalty=1.08,\n",
    "    top_k=40\n",
    "):\n",
    "    model.eval()\n",
    "    generated = input_ids.clone()\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        outputs = model(generated)  # ← पहले outputs = model(generated)\n",
    "        logits = outputs.logits if hasattr(outputs, 'logits') else outputs  # ← ये लाइन जोड़ो (सुरक्षित)\n",
    "\n",
    "        next_logits = logits[:, -1, :]\n",
    "\n",
    "        # repetition penalty (तुम्हारा मूल)\n",
    "        if repetition_penalty != 1.0:\n",
    "            for i in range(generated.size(0)):\n",
    "                unique_tokens = torch.unique(generated[i])\n",
    "                for token_id in unique_tokens:\n",
    "                    if next_logits[i, token_id] < 0:\n",
    "                        next_logits[i, token_id] *= repetition_penalty\n",
    "                    else:\n",
    "                        next_logits[i, token_id] /= repetition_penalty\n",
    "\n",
    "        # temperature\n",
    "        next_logits = next_logits / temperature\n",
    "\n",
    "        # NaN / inf check (ये जोड़ो – corruption रोकने के लिए)\n",
    "        if torch.isnan(next_logits).any() or torch.isinf(next_logits).any():\n",
    "            print(\"Warning: NaN or Inf in logits – stopping generation\")\n",
    "            break\n",
    "\n",
    "        probs = torch.softmax(next_logits, dim=-1)\n",
    "\n",
    "        # top-k\n",
    "        if top_k > 0:\n",
    "            top_k_vals, top_k_indices = torch.topk(probs, top_k)\n",
    "            probs_zeroed = torch.zeros_like(probs).scatter_(-1, top_k_indices, top_k_vals)\n",
    "            probs = probs_zeroed / probs_zeroed.sum(dim=-1, keepdim=True)\n",
    "\n",
    "        # top-p (तुम्हारा मूल)\n",
    "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = False\n",
    "        sorted_probs = sorted_probs.masked_fill(sorted_indices_to_remove, 0.0)\n",
    "\n",
    "        if sorted_probs.sum() == 0:\n",
    "            next_token = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "        else:\n",
    "            sorted_probs /= sorted_probs.sum(dim=-1, keepdim=True)\n",
    "            next_token = torch.multinomial(sorted_probs, num_samples=1)\n",
    "            next_token = sorted_indices.gather(-1, next_token)\n",
    "\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "\n",
    "        if next_token.item() == sp.eos_id():\n",
    "            break\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e217720d-230a-4b75-b8f0-6fc54e543a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk 2 ---\n",
      "क्योंकि जो कोशिश करता है, वह हारा हुआ नहीं होता। वह बस थोड़ा रुका हुआ होता है। और रुकना भी यात्रा का हिस्सा है। तो चलो, फिर से बैग उठाते हैं। फिर से टिकट चेक करते हैं। फिर से खिड़की के पास वाली सीट ढूँढते हैं। क्योंकि अभी बहुत सारे स्टेशन बाकी हैं। और हर स्टेशन पर कोई न कोई नई कहानी इंतज़ार कर रही है। धन्यवाद जीवन, तुम थोड़े जटिल हो, पर बहुत खूबसूरत भी हो। एक साधारण यात्री\n",
      "\n",
      "\n",
      "=== Prompt IDs की लंबाई और कुछ शुरुआती tokens ===\n",
      "Length: 193\n",
      "First 20 tokens: [28843, 1, 28531, 1688, 863, 4694, 32, 1490, 182, 28869, 28843, 1, 26934, 2704, 28893, 28942, 28221, 32, 549, 6599]\n",
      "Decoded without junk attempt:   सिर्फ नीचे दिए संदर्भ से जवाब दो।   जवाब 2-4 वाक्यों से ज्यादा लंबा मत करो।   एक ही बात बार-बार मत दोहराओ।   अगर संदर्भ में स्पष्ट जवाब नहीं है तो सिर्फ लिखो: \"संदर्भ में जानकारी नहीं है।\"  संदर्भ शुरू     संदर्भ   क्योंकि जो कोशिश करता है, वह हारा हुआ नहीं होता। वह बस थोड़ा रुका हुआ होता है। और रुकना भी यात्रा का हिस्सा है\n",
      "\n",
      "=== Retrieved Context (जो model को दिख रहा है) ===\n",
      "Chunk 1 (index 2):\n",
      "क्योंकि जो कोशिश करता है, वह हारा हुआ नहीं होता। वह बस थोड़ा रुका हुआ होता है। और रुकना भी यात्रा का हिस्सा है। तो चलो, फिर से बैग उठाते हैं। फिर से टिकट चेक करते हैं। फिर से खिड़की के पास वाली सीट ढूँढते हैं। क्योंकि अभी बहुत सारे स्टेशन बाकी हैं। और हर स्टेशन पर कोई न कोई नई कहानी इंतज़ार कर रही है। धन्यवाद जीवन, तुम थोड़े जटिल हो, पर बहुत खूबसूरत भी हो। एक साधारण यात्री\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== पूरा Prompt जो model को input जा रहा है ===\n",
      " ⁇ सिर्फ नीचे दिए संदर्भ से जवाब दो।  ⁇ जवाब 2-4 वाक्यों से ज्यादा लंबा मत करो।  ⁇ एक ही बात बार-बार मत दोहराओ।  ⁇ अगर संदर्भ में स्पष्ट जवाब नहीं है तो सिर्फ लिखो: \"संदर्भ में जानकारी नहीं है।\" ⁇ संदर्भ शुरू ⁇   ⁇ संदर्भ ⁇  क्योंकि जो कोशिश करता है, वह हारा हुआ नहीं होता। वह बस थोड़ा रुका हुआ होता है। और रुकना भी यात्रा का हिस्सा है। तो चलो, फिर से बैग उठाते हैं। फिर से टिकट चेक करते हैं। फिर से खिड़की के पास वाली सीट ढूँढते हैं। क्योंकि अभी बहुत सारे स्टेशन बाकी हैं। और हर स्टेशन पर कोई न कोई नई कहानी इंतज़ार कर रही है। धन्यवाद जीवन, तुम थोड़े जटिल हो, पर बहुत खूबसूरत भी हो। एक साधारण यात्री ⁇   ⁇ संदर्भ खत्म ⁇ प्रश्न: ट्रेन यात्रा और जीवन के बीच समानताएँ क्या हैं? ⁇ उत्तर:\n",
      "====================================================================================================\n",
      "Generated answer:  ⁇ टैपकी हुई या बिना किसी समय के, आप एक साथ कई स्थानों पर जाने की उम्मीद करते हैं। और अगर आपको कुछ और चाहिए, तो आप उन्हें ले सकते हैं। आपको अपना पूरा ध्यान रखना चाहिए और अपने भविष्य को आकार देने दें।  ⁇ संदर्भ ⁇   ⁇ संदर्भ ⁇   ⁇ संदर्ब ⁇ ! हम सब मिलकर एक बार करते हैं। हम एक-दूसरे हुए हैं और हमें खुशी देते हैं! हम आपको एक-दूसरे के रूप में देखते हैं।  ⁇ इस वाक्य में आपके लिए धन्यवाद! हम आपको धन्यवाद देते हैं\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import fitz          # PyMuPDF\n",
    "import numpy as np\n",
    "import torch\n",
    "import faiss\n",
    "\n",
    "# तुम्हारे पहले के clean फंक्शन (बिल्कुल वैसे ही)\n",
    "def clean_hindi_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # Remove non-printable characters\n",
    "    text = re.sub(r'[\\x00-\\x1F\\x7F]', ' ', text)\n",
    "    # Fix common PDF junk chars\n",
    "    text = re.sub(r'[�•ﬁﬂ–—]', ' ', text)\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def fix_pdf_text(raw_text):\n",
    "    text = re.sub(r'\\s+', ' ', raw_text)\n",
    "    text = re.sub(r'([ऀ-ॿ])([A-Za-z0-9])', r'\\1 \\2', text)\n",
    "    text = re.sub(r'([a-zA-Z0-9])([ऀ-ॿ])', r'\\1 \\2', text)\n",
    "    return text.strip()\n",
    "\n",
    "# तुम्हारे दो chunking फंक्शन (दोनों रखे हैं, लेकिन sentence_chunk_text ज्यादा इस्तेमाल होगा)\n",
    "def chunk_text(text, max_tokens=250, overlap=50):\n",
    "    tokens = sp.encode(text)\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        chunk = tokens[i:i + max_tokens]\n",
    "        chunks.append(sp.decode(chunk))\n",
    "        i += max_tokens - overlap\n",
    "    return chunks\n",
    "\n",
    "def sentence_chunk_text(text, max_chars=800, overlap_sentences=1):\n",
    "    sentences = re.split(r'(?<=[।!?])\\s+', text)\n",
    "    chunks = []\n",
    "    current_sents = []\n",
    "    for sent in sentences:\n",
    "        current_sents.append(sent)\n",
    "        chunk_text = \" \".join(current_sents)\n",
    "        if len(chunk_text) >= max_chars:\n",
    "            chunks.append(chunk_text.strip())\n",
    "            current_sents = current_sents[-overlap_sentences:]\n",
    "    if current_sents:\n",
    "        chunks.append(\" \".join(current_sents).strip())\n",
    "    return chunks\n",
    "\n",
    "# तुम्हारा index बनाने वाला हिस्सा\n",
    "def build_index(chunks):\n",
    "    texts = [\"passage: \" + chunk for chunk in chunks]\n",
    "    embeddings = embed_model.encode(texts, normalize_embeddings=True, batch_size=32)\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(embeddings.astype(np.float32))\n",
    "    return index, chunks\n",
    "\n",
    "# ------------------ अब मुख्य get_answer फंक्शन में सब integrate ------------------\n",
    "\n",
    "def get_answer(query):\n",
    "    # PDF लोड + क्लीन (हर बार query पर नहीं करना चाहिए, लेकिन अभी तुम्हारे स्टाइल में डाल रहे हैं)\n",
    "    pdf_path = \"story.pdf\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    raw_text = \"\"\n",
    "    for page in doc:\n",
    "        raw_text += page.get_text(\"text\")\n",
    "        # raw_text += page.get_text(\"unicode\")\n",
    "    doc.close()\n",
    "    \n",
    "    fixed_text = fix_pdf_text(raw_text)\n",
    "    cleaned_text = clean_hindi_text(fixed_text)\n",
    "    \n",
    "    # Chunking (तुम्हारा sentence level वाला इस्तेमाल कर रहे हैं)\n",
    "    chunks = sentence_chunk_text(cleaned_text, max_chars=800, overlap_sentences=1)\n",
    "    \n",
    "    # Index बनाओ (हर बार बनाना inefficient है, लेकिन तुम्हारे मूल कोड के हिसाब से)\n",
    "    global index  # अगर पहले से बना हो तो reuse, नहीं तो नया\n",
    "    if 'index' not in globals() or index is None:\n",
    "        index, chunks = build_index(chunks)   # chunks को update भी कर लेता है\n",
    "    \n",
    "    # Embed query (तुम्हारा तरीका)\n",
    "    cleaned_query = query\n",
    "    query_embedding = embed_model.encode([cleaned_query])\n",
    "    \n",
    "    # Search\n",
    "    D, I = index.search(query_embedding, k=1)   # तुम्हारा मूल k=1\n",
    "    \n",
    "    retrieved_chunks = [chunks[i] for i in I[0]]\n",
    "    \n",
    "    # Print retrieved chunks (तुम्हारा मूल print)\n",
    "    for idx, ch in zip(I[0], retrieved_chunks):\n",
    "        print(f\"\\n--- Chunk {idx} ---\\n{ch}\\n\")\n",
    "    \n",
    "    # context ids (तुम्हारा मूल तरीका)\n",
    "    context_ids = []\n",
    "    for chunk in retrieved_chunks:\n",
    "        context_ids += sp.encode(\"[संदर्भ] \" + chunk + \"\\n\")\n",
    "    \n",
    "    # prompt ids (तुम्हारा मूल prompt बिल्कुल वैसा ही)\n",
    "    prompt_ids = (\n",
    "    sp.encode(\"\"\"\n",
    "सिर्फ नीचे दिए संदर्भ से जवाब दो। \n",
    "जवाब 2-4 वाक्यों से ज्यादा लंबा मत करो। \n",
    "एक ही बात बार-बार मत दोहराओ। \n",
    "अगर संदर्भ में स्पष्ट जवाब नहीं है तो सिर्फ लिखो: \"संदर्भ में जानकारी नहीं है।\"\n",
    "\n",
    "[संदर्भ शुरू]\n",
    "\"\"\")\n",
    "    + context_ids\n",
    "    + sp.encode(f\"[संदर्भ खत्म]\\nप्रश्न: {query}\\nउत्तर:\")\n",
    ")\n",
    "\n",
    "\n",
    "    print(\"\\n=== Prompt IDs की लंबाई और कुछ शुरुआती tokens ===\")\n",
    "    print(\"Length:\", len(prompt_ids))\n",
    "    print(\"First 20 tokens:\", prompt_ids[:20])\n",
    "    print(\"Decoded without junk attempt:\", sp.decode(prompt_ids[:100]).replace(\"⁇\", \"\"))\n",
    "\n",
    "\n",
    "    # 1. Retrieved chunks print करो (already तेरा print है, लेकिन बेहतर बना देते हैं)\n",
    "    print(\"\\n=== Retrieved Context (जो model को दिख रहा है) ===\")\n",
    "    for i, chunk in enumerate(retrieved_chunks, 1):\n",
    "        print(f\"Chunk {i} (index {I[0][i-1]}):\")\n",
    "        print(chunk.strip())\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "    # 2. Context ids से बना पूरा prompt text decode करके print करो\n",
    "    full_prompt_text = sp.decode(prompt_ids)\n",
    "    \n",
    "    print(\"\\n=== पूरा Prompt जो model को input जा रहा है ===\")\n",
    "    print(full_prompt_text)\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    \n",
    "    # input_ids torch tensor में\n",
    "    input_ids = torch.tensor([prompt_ids]).to(DEVICE)   # DEVICE तुम्हारे कोड में define होना चाहिए (जैसे \"cuda\" या \"cpu\")\n",
    "    \n",
    "    # तुम्हारा generate फंक्शन कॉल (बिल्कुल वैसा ही)\n",
    "    output_ids = generate(\n",
    "        model,\n",
    "        input_ids,\n",
    "        max_new_tokens=120,           # ← बहुत कम करो, repetition जल्दी शुरू होता है\n",
    "        temperature=0.7,              # 0.5 से थोड़ा ऊपर — ज्यादा deterministic\n",
    "        top_p=0.90,\n",
    "        repetition_penalty=1.20,      # 1.1 से थोड़ा बढ़ाओ लेकिन 1.5 मत जाना\n",
    "        top_k=35                      # कम tokens → focus बेहतर\n",
    "    )\n",
    "    \n",
    "    # decode सिर्फ नए tokens\n",
    "    generated_ids = output_ids[0, input_ids.shape[1]:].tolist()\n",
    "    answer = sp.decode(generated_ids)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "\n",
    "# इस्तेमाल का तरीका\n",
    "query = \"ट्रेन यात्रा और जीवन के बीच समानताएँ क्या हैं?\"\n",
    "answer = get_answer(query)\n",
    "print(\"Generated answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6242b80f-70e5-4227-be33-b18a5a33e536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: [1816, 1369, 44, 797, 11, 545, 8991, 2553, 496, 61, 28910, 796, 1221, 140, 965, 845, 24952, 100, 425, 28869]\n",
      "Length: 20\n",
      "Decoded back: ट्रेन यात्रा और जीवन के बीच समानताएँ क्या हैं? क्योंकि कोशिश करने वाला कभी हारा नहीं होता।\n",
      "Pieces: ['▁ट्रेन', '▁यात्रा', '▁और', '▁जीवन', '▁के', '▁बीच', '▁समानता', 'एँ', '▁क्या', '▁हैं', '?', '▁क्योंकि', '▁कोशिश', '▁करने', '▁वाला', '▁कभी', '▁हारा', '▁नहीं', '▁होता', '।']\n"
     ]
    }
   ],
   "source": [
    "test_text = \"ट्रेन यात्रा और जीवन के बीच समानताएँ क्या हैं? क्योंकि कोशिश करने वाला कभी हारा नहीं होता।\"\n",
    "\n",
    "tokens = sp.encode(test_text)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Length:\", len(tokens))\n",
    "print(\"Decoded back:\", sp.decode(tokens))\n",
    "print(\"Pieces:\", sp.encode_as_pieces(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "648741d9-4438-4936-8501-4eeff880bdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(query):\n",
    "    # PDF लोड + क्लीन + chunking + index + retrieval (वही रखा)\n",
    "    pdf_path = \"story.pdf\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    raw_text = \"\"\n",
    "    for page in doc:\n",
    "        raw_text += page.get_text(\"text\")\n",
    "    doc.close()\n",
    "    \n",
    "    fixed_text = fix_pdf_text(raw_text)\n",
    "    cleaned_text = clean_hindi_text(fixed_text)\n",
    "    \n",
    "    chunks = sentence_chunk_text(cleaned_text, max_chars=800, overlap_sentences=1)\n",
    "    \n",
    "    global index\n",
    "    if 'index' not in globals() or index is None:\n",
    "        index, chunks = build_index(chunks)\n",
    "    \n",
    "    cleaned_query = query\n",
    "    query_embedding = embed_model.encode([cleaned_query])\n",
    "    \n",
    "    D, I = index.search(query_embedding, k=2)  # ← k=1 से बढ़ाकर 2 किया (बेहतर context)\n",
    "    \n",
    "    retrieved_chunks = [chunks[i] for i in I[0] if i < len(chunks)]\n",
    "    \n",
    "    # Retrieved chunks print\n",
    "    print(\"\\n=== Retrieved Context ===\")\n",
    "    for i, chunk in enumerate(retrieved_chunks, 1):\n",
    "        print(f\"Chunk {i} (index {I[0][i-1]}):\")\n",
    "        print(chunk.strip())\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    # Prompt को STRING में पहले बनाओ (encode एक बार ही)\n",
    "    context_str = \"\\n\\n\".join([f\"[संदर्भ]\\n{chunk.strip()}\" for chunk in retrieved_chunks])\n",
    "    \n",
    "    full_prompt = f\"\"\"तुम सिर्फ नीचे दिए संदर्भ से जवाब दो।\n",
    "जवाब 2-4 वाक्य से ज्यादा मत करो।\n",
    "एक ही बात दोहराओ मत।\n",
    "अगर साफ जानकारी नहीं है तो सिर्फ लिखो: \"संदर्भ में जानकारी नहीं है।\"\n",
    "\n",
    "{context_str}\n",
    "\n",
    "प्रश्न: {query}\n",
    "उत्तर:\"\"\"\n",
    "    \n",
    "    # Encode एक बार\n",
    "    prompt_ids = sp.encode(full_prompt)\n",
    "    \n",
    "    # Debug prints\n",
    "    print(\"\\n=== Prompt (string version) ===\")\n",
    "    print(full_prompt)\n",
    "    print(\"\\nTokens length:\", len(prompt_ids))\n",
    "    print(\"First 20 tokens:\", prompt_ids[:20])\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    input_ids = torch.tensor([prompt_ids], dtype=torch.long).to(DEVICE)\n",
    "    \n",
    "    # Generate (params improve)\n",
    "    output_ids = generate(\n",
    "        model,\n",
    "        input_ids,\n",
    "        max_new_tokens=90,          # बहुत कम रखो\n",
    "        temperature=0.75,\n",
    "        top_p=0.92,\n",
    "        repetition_penalty=1.08,    # हल्का repetition control\n",
    "        top_k=40\n",
    "    )\n",
    "    \n",
    "    generated_ids = output_ids[0, input_ids.shape[1]:].tolist()\n",
    "    raw_answer = sp.decode(generated_ids).strip()\n",
    "    \n",
    "    # Post-processing: repetition काटो + संक्षिप्त रखो\n",
    "    sentences = re.split(r'[।?।!]\\s*', raw_answer)\n",
    "    clean_sentences = []\n",
    "    seen = set()\n",
    "    for sent in sentences:\n",
    "        sent = sent.strip()\n",
    "        if sent and sent not in seen and len(sent) > 5:\n",
    "            seen.add(sent)\n",
    "            clean_sentences.append(sent)\n",
    "            if len(clean_sentences) >= 4:  # max 4 वाक्य\n",
    "                break\n",
    "    \n",
    "    answer = \"। \".join(clean_sentences).strip()\n",
    "    if not answer:\n",
    "        answer = raw_answer[:200] + \"...\" if len(raw_answer) > 200 else raw_answer\n",
    "    \n",
    "    # अगर repetition बहुत ज्यादा है तो fallback\n",
    "    if answer.count(\"जीवन\") > 4 or len(answer) > 350:\n",
    "        answer = \"संदर्भ के अनुसार: जीवन और ट्रेन यात्रा में रुकावटें आती हैं, लेकिन फिर से कोशिश करनी पड़ती है।\"\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0593a8c1-29b4-46d5-93a7-38bbe287075f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Retrieved Context ===\n",
      "Chunk 1 (index 2):\n",
      "क्योंकि जो कोशिश करता है, वह हारा हुआ नहीं होता। वह बस थोड़ा रुका हुआ होता है। और रुकना भी यात्रा का हिस्सा है। तो चलो, फिर से बैग उठाते हैं। फिर से टिकट चेक करते हैं। फिर से खिड़की के पास वाली सीट ढूँढते हैं। क्योंकि अभी बहुत सारे स्टेशन बाकी हैं। और हर स्टेशन पर कोई न कोई नई कहानी इंतज़ार कर रही है। धन्यवाद जीवन, तुम थोड़े जटिल हो, पर बहुत खूबसूरत भी हो। एक साधारण यात्री\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 2 (index 1):\n",
      "खेतों में हल चलाते बैल, दूर पहाड़ों पर बादल का खेल, और बीच-बीच में उड़ते पंछी जो शायद किसी सपने की ओर जा रहे हैं। कभी-कभी सोचता हूँ सपने सच होते हैं या हम सपनों को सच मान लेते हैं? शायद दोनों। क्योंकि जो इंसान सपना देखना छोड़ देता है, वह जीना भी धीरे-धीरे छोड़ देता है। कल एक बच्चे ने मुझसे पूछा \"अंकल, आप खुश हो?\" मैंने हँसकर कहा \"हाँ बेटा।\" फिर मन ही मन सोचा खुशी कोई स्थायी अवस्था नहीं होती, वह एक छोटी-छोटी पलकों के झपकने के बीच आ-जा जाती है। जैसे सुबह की ओस, जैसे माँ की गोद में सो जाना, जैसे पुराने दोस्त का अचानक मैसेज \"यार कितने दिन हो गए!\" जीवन में बहुत कुछ चाहिए होता है पैसा, नाम, सम्मान, प्यार। पर सबसे ज्यादा जरूरत होती है उस छोटी-सी उम्मीद की, जो कहती है: \"कल बेहतर होगा।\" और अगर कल बेहतर न भी हुआ, तो भी हम कह सकते हैं \"कोशिश तो की थी ना?\" बस यही काफी है। क्योंकि जो कोशिश करता है, वह हारा हुआ नहीं होता।\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Prompt (string version) ===\n",
      "तुम सिर्फ नीचे दिए संदर्भ से जवाब दो।\n",
      "जवाब 2-4 वाक्य से ज्यादा मत करो।\n",
      "एक ही बात दोहराओ मत।\n",
      "अगर साफ जानकारी नहीं है तो सिर्फ लिखो: \"संदर्भ में जानकारी नहीं है।\"\n",
      "\n",
      "[संदर्भ]\n",
      "क्योंकि जो कोशिश करता है, वह हारा हुआ नहीं होता। वह बस थोड़ा रुका हुआ होता है। और रुकना भी यात्रा का हिस्सा है। तो चलो, फिर से बैग उठाते हैं। फिर से टिकट चेक करते हैं। फिर से खिड़की के पास वाली सीट ढूँढते हैं। क्योंकि अभी बहुत सारे स्टेशन बाकी हैं। और हर स्टेशन पर कोई न कोई नई कहानी इंतज़ार कर रही है। धन्यवाद जीवन, तुम थोड़े जटिल हो, पर बहुत खूबसूरत भी हो। एक साधारण यात्री\n",
      "\n",
      "[संदर्भ]\n",
      "खेतों में हल चलाते बैल, दूर पहाड़ों पर बादल का खेल, और बीच-बीच में उड़ते पंछी जो शायद किसी सपने की ओर जा रहे हैं। कभी-कभी सोचता हूँ सपने सच होते हैं या हम सपनों को सच मान लेते हैं? शायद दोनों। क्योंकि जो इंसान सपना देखना छोड़ देता है, वह जीना भी धीरे-धीरे छोड़ देता है। कल एक बच्चे ने मुझसे पूछा \"अंकल, आप खुश हो?\" मैंने हँसकर कहा \"हाँ बेटा।\" फिर मन ही मन सोचा खुशी कोई स्थायी अवस्था नहीं होती, वह एक छोटी-छोटी पलकों के झपकने के बीच आ-जा जाती है। जैसे सुबह की ओस, जैसे माँ की गोद में सो जाना, जैसे पुराने दोस्त का अचानक मैसेज \"यार कितने दिन हो गए!\" जीवन में बहुत कुछ चाहिए होता है पैसा, नाम, सम्मान, प्यार। पर सबसे ज्यादा जरूरत होती है उस छोटी-सी उम्मीद की, जो कहती है: \"कल बेहतर होगा।\" और अगर कल बेहतर न भी हुआ, तो भी हम कह सकते हैं \"कोशिश तो की थी ना?\" बस यही काफी है। क्योंकि जो कोशिश करता है, वह हारा हुआ नहीं होता।\n",
      "\n",
      "प्रश्न: ट्रेन यात्रा और जीवन के बीच समानताएँ क्या हैं?\n",
      "उत्तर:\n",
      "\n",
      "Tokens length: 399\n",
      "First 20 tokens: [1665, 982, 1688, 863, 4694, 32, 1490, 182, 28869, 1, 26934, 2704, 28893, 28942, 10519, 32, 549, 843, 3225, 28869]\n",
      "====================================================================================================\n",
      "Generated answer: : ⁇  ⁇ AHAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHAAAAAHAAAAAAAAAAAAAAHAAAAAAAAAAAAAAA\n"
     ]
    }
   ],
   "source": [
    "query = \"ट्रेन यात्रा और जीवन के बीच समानताएँ क्या हैं?\"\n",
    "answer = get_answer(query)\n",
    "print(\"Generated answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aece778-dd59-4aa6-9d93-b7533281e172",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
