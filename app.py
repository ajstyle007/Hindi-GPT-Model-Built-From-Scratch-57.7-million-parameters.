import gradio as gr
import torch, re
import sentencepiece as spm
from sft_gen import generate
from decoder_only_gpt import My_GPT_model

# ------------------ Load tokenizer ------------------
sp = spm.SentencePieceProcessor()
sp.load("hindi_tokenizer_new.model")

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ------------------ Load model ------------------
model = My_GPT_model(
    vocab_size=sp.get_piece_size(),
    num_layers=12,
    d_model=512,
    d_ff=2048,
    num_heads=8,
    seq_len=512
).to(DEVICE)

model.load_state_dict(torch.load("full_sft_final.pt", map_location=DEVICE))
model.eval()

# ------------------ Helpers ------------------
def encode_text(text, max_len=512):
    ids = sp.encode(text, out_type=int)[:max_len]
    return torch.tensor([ids], device=DEVICE)

def decode_tokens(token_ids):
    return sp.decode(token_ids[0].tolist())

def post_clean(text):
    text = text.replace("тБЗ", "")
    text = re.sub(r"\s+", " ", text)
    return text.strip()


sample_questions = ["рдЗрдЯрд░рдиреЗрдЯ рдХреИрд╕реЗ рдХрд╛рдо рдХрд░рддрд╛ рд╣реИ?",
"рд▓реЛрдХрддрдВрддреНрд░ рдХреНрдпрд╛ рд╣реИ?",
"рдЧреБрдирд╛рд╣реЛрдВ рдХрд╛ рджреЗрд╡рддрд╛ рдЙрдкрдиреНрдпрд╛рд╕ рдХрд┐рд╕рдиреЗ рд▓рд┐рдЦрд╛?",
"рдорд╢реАрди рд▓рд░реНрдирд┐рдВрдЧ рдХреНрдпрд╛ рд╣реИ?",
"рдорд╣рд╛рддреНрдорд╛ рдЧрд╛рдВрдзреА рдХреМрди рдереЗ?",
"1857 рдХреА рдХреНрд░рд╛рдВрддрд┐ рдХреНрдпрд╛ рдереА?",
"рднрд╛рд░рдд рдХреА рд░рд╛рдЬрдзрд╛рдиреА рдХреНрдпрд╛ рд╣реИ?",
"рдПрдХ рд░рдЪрдирд╛рддреНрдордХ рдХрд╣рд╛рдиреА рд▓рд┐рдЦрд┐рдПред",
"рдЕрдЧрд░ рдЖрдкрдХреЛ рдмрд┐рдирд╛ рдЗрдВрдЯрд░рдиреЗрдЯ рдХреЗ 24 рдШрдВрдЯреЗ рдмрд┐рддрд╛рдиреЗ рдкрдбрд╝реЗрдВ, рддреЛ рдЖрдк рдЙрд╕ рд╕рдордп рдХреНрдпрд╛-рдХреНрдпрд╛ рдХрд░реЗрдВрдЧреЗ?",
"рдЕрдЧрд░ AI рдЗрдВрд╕рд╛рдиреЛрдВ рдХреА рддрд░рд╣ рд╕реЛрдЪрдиреЗ рд▓рдЧреЗ, рддреЛ рд╕рдмрд╕реЗ рдкрд╣рд▓реЗ рдХреМрди-рд╕реА рдЪреАрдЬрд╝ рдмрджрд▓реЗрдЧреА?",
"рдХреНрдпрд╛ рдкреИрд╕рд╛ рдЦреБрд╢реА рдЦрд░реАрдж рд╕рдХрддрд╛ рд╣реИ?",
"рд╕рдлрд▓рддрд╛ рднрд╛рдЧреНрдп рд╕реЗ рдорд┐рд▓рддреА рд╣реИ рдпрд╛ рдореЗрд╣рдирдд рд╕реЗ?",
"рдХреНрдпрд╛ рдЗрдВрд╕рд╛рди рдХрднреА рдЕрдорд░ рд╣реЛ рд╕рдХрддрд╛ рд╣реИ?",
"рдХреНрдпрд╛ рдмреНрд░рд╣реНрдорд╛рдВрдб рдкреВрд░реА рддрд░рд╣ рдкреВрд░реНрд╡-рдирд┐рд░реНрдзрд╛рд░рд┐рдд рд╣реИ?",
"рдХреНрдпрд╛ рд╕рдордп рдПрдХ рднреНрд░рдо рд╣реИ?",
"рднрдЧрд╡рд╛рди рдХреЛ рдХрд┐рд╕рдиреЗ рдмрдирд╛рдпрд╛?",
"рдЕрдЧрд░ рднрдЧрд╡рд╛рди рдорд░ рдЧрдпрд╛, рддреЛ рдиреИрддрд┐рдХрддрд╛ рдХрд┐рд╕рдХреА рд╣реЛрдЧреА?"
]

# ------------------ Gradio function ------------------
@torch.no_grad()
def gradio_wrapper(query):
    if not query.strip():
        return "рдХреГрдкрдпрд╛ рдкреНрд░рд╢реНрди рд▓рд┐рдЦреЗрдВред"

    prompt = f"### рдкреНрд░рд╢реНрди:\n{query}\n\n### рдЙрддреНрддрд░:\n"


    input_ids = encode_text(prompt)

    with torch.autocast("cuda", dtype=torch.bfloat16):
        output_ids = generate(
            model,
            input_ids,
            max_new_tokens=300,
            temperature=0.85,
            top_p=0.92,
            top_k=45,
            repetition_penalty=1.12,
            eos_token_id=sp.eos_id(),
            pad_token_id=sp.bos_id()
        )

    answer = decode_tokens(output_ids)
    answer = post_clean(answer)

    # Optional: sirf answer part return karo
    if "### рдЙрддреНрддрд░:" in answer:
        answer = answer.split("### рдЙрддреНрддрд░:")[-1].strip()

    return answer

custom_css = """
/* Examples container рдХреЛ target рдХрд░реЛ (рдЧреНрд░реБрдк рдореЗрдВ рд░рд╣рддрд╛ рд╣реИ) */
/* Example container */
.gradio-container div[data-testid="examples"] {
    width: 30%;
}

/* Example items vertical */
.gradio-container div[data-testid="examples"] > div {
    flex-direction: column;
}

/* рд╣рд░ example card рдХреЛ adjust */
.gradio-container .examples .example {
    width: 100% !important;
    margin-bottom: 8px !important;
    padding: 8px !important;
    border: 1px solid #ddd !important;
    border-radius: 6px !important;
}

"""

# ------------------ Gradio UI ------------------
demo = gr.Interface(
    fn=gradio_wrapper,
    inputs=gr.Textbox(
        lines=3,
        placeholder="рдЕрдкрдирд╛ рдкреНрд░рд╢реНрди рдпрд╣рд╛рдБ рд▓рд┐рдЦреЗрдВ...",
        label="рдкреНрд░рд╢реНрди"
    ),
    outputs=gr.Textbox(
        lines=10,
        label="рдЙрддреНрддрд░"
    ),
    description="Fine-tuned Hindi GPT рдЖрдзрд╛рд░рд┐рдд рдкреНрд░рд╢реНрди-рдЙрддреНрддрд░ рдкреНрд░рдгрд╛рд▓реА",
    examples=sample_questions,
    css=custom_css,          # тЖР рдореБрдЦреНрдп hack рдпрд╣рд╛рдБ
    cache_examples=False,
)


with gr.Blocks(css=custom_css) as demo:

    gr.Markdown(
        """
        <h1 style="
            text-align:center;
            margin-top:40px;
            font-size:clamp(25px, 3vw, 40px);
            font-weight:700;
            color:white;
        ">
        тЭУ рд╣рд┐рдВрджреА GPT<span style="color:#0EA5E9;"> рдкреНрд░рд╢реНрди-рдЙрддреНрддрд░</span>
        </h1>
        <p style="color:gray; text-align:center; margin:25px 0 10px 0;">
        Fine-tuned Hindi GPT рдЖрдзрд╛рд░рд┐рдд рдкреНрд░рд╢реНрди-рдЙрддреНрддрд░ рдкреНрд░рдгрд╛рд▓реА
        </p>
        """
    )

    gr.Interface(
        fn=gradio_wrapper,
        inputs=gr.Textbox(
            lines=3,
            placeholder="рдЕрдкрдирд╛ рдкреНрд░рд╢реНрди рдпрд╣рд╛рдБ рд▓рд┐рдЦреЗрдВ...",
            label="рдкреНрд░рд╢реНрди"
        ),
        outputs=gr.Textbox(
            lines=10,
            label="рдЙрддреНрддрд░"
        ),
        examples=sample_questions,
        cache_examples=False,
    )


demo.launch(debug=True)

# demo = gr.Interface(
#     fn=gradio_wrapper,
#     inputs=gr.Textbox(lines=3, placeholder="рдЕрдкрдирд╛ рдкреНрд░рд╢реНрди рдпрд╣рд╛рдБ рд▓рд┐рдЦреЗрдВ...", label="рдкреНрд░рд╢реНрди"),
#     outputs=gr.Textbox(lines=6, label="рдЙрддреНрддрд░"),
#     title="ЁЯУШ Hindi Question Answering (SFT GPT)",
#     description="Fine-tuned Hindi GPT рдЖрдзрд╛рд░рд┐рдд рдкреНрд░рд╢реНрди-рдЙрддреНрддрд░ рдкреНрд░рдгрд╛рд▓реА\n\nрдиреАрдЪреЗ рдЙрджрд╛рд╣рд░рдг рджреЗрдЦреЗрдВ тЖУ",
#     examples=[ [q] for q in sample_questions ],   # list of lists рдЬрд░реВрд░реА
#     examples_per_page=8,
#     cache_examples=False,
# )
# demo.launch(debug=True)
