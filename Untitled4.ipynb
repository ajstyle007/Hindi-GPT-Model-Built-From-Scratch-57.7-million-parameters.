{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65d7ca69-2868-486b-8a1f-cd9e536f2bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import faiss\n",
    "import sentencepiece as spm\n",
    "import fitz  # PyMuPDF\n",
    "from decoder_only_gpt import My_GPT_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6a6220b-575d-44b7-9641-f7b17c913368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"hindi_tokenizer_new.model\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c33b027-3e56-49c0-88cd-6c85b77ec405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e0d2b5d-72ba-4ffb-944c-405e9d6f97c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = My_GPT_model(vocab_size=sp.get_piece_size(), num_layers=12,\n",
    "    d_model=512, d_ff=2048, num_heads=8,seq_len=512\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abec0ceb-882d-4973-af63-f8485b995d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(\"checkpoints_HindiGPT-v1_step280000.pt\", map_location=DEVICE)\n",
    "state_dict = ckpt[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "892dc133-e0cd-4420-b058-b1c522ae6445",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_state_dict = {k.replace(\"_orig_mod.\", \"\"): v for k, v in state_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "520b92bc-aaa2-4c9b-8032-184b66e93a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "My_GPT_model(\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(32768, 512)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x Decoder_GPT_Block(\n",
       "        (swi_glu): SwiGLU_FFN(\n",
       "          (w1): Linear(in_features=512, out_features=1536, bias=False)\n",
       "          (w2): Linear(in_features=512, out_features=1536, bias=False)\n",
       "          (w3): Linear(in_features=1536, out_features=512, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (masked_mha): Masked_MHA(\n",
       "          (Q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (K): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (V): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (rms_norm0): RMSNorm()\n",
       "        (rms_norm1): RMSNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(clean_state_dict, strict=False)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbe0a697-72d9-4729-ae1d-7783cab153c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embed_model = SentenceTransformer(\n",
    "    \"intfloat/multilingual-e5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b41411be-7103-41e2-8acc-6f8bae5d41d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "reranker = CrossEncoder(\"cross-encoder/mmarco-mMiniLMv2-L12-H384-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ced65c93-4103-4a98-b89a-f9c6d6e31199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_faiss_index(chunks):\n",
    "    embeddings = embed_model.encode(\n",
    "        [\"passage: \" + t for t in chunks],\n",
    "        normalize_embeddings=True,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "    index.add(embeddings)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2862d166-b3fd-412b-9b28-7e75b894a621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(query, chunks, index, top_k=10, rerank_top=3, min_score=0.5):\n",
    "    query_emb = embed_model.encode([\"query: \" + query], normalize_embeddings=True)\n",
    "    scores, idxs = index.search(query_emb, top_k)\n",
    "    candidate_texts = []\n",
    "    for i, score in zip(idxs[0], scores[0]):\n",
    "        if score < min_score:\n",
    "            continue\n",
    "        candidate_texts.append(chunks[i])\n",
    "    \n",
    "    if not candidate_texts:\n",
    "        return []\n",
    "    \n",
    "    # Rerank top candidates\n",
    "    reranked = rerank(query, candidate_texts, top_k=rerank_top)\n",
    "    context_ids = []\n",
    "    for t in reranked:\n",
    "        context_ids += sp.encode(f\"\\n[संदर्भ]\\n{t}\\n\")\n",
    "    \n",
    "    # Limit context to ~400 tokens to fit model\n",
    "    return context_ids[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c46ced0-86de-43f8-87fa-495a97220154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank(query, candidate_texts, top_k=3):\n",
    "    pairs = [(query, t) for t in candidate_texts]\n",
    "    scores = reranker.predict(pairs)\n",
    "    ranked = sorted(zip(candidate_texts, scores), key=lambda x: x[1], reverse=True)\n",
    "    return [x[0] for x in ranked[:top_k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84840cf-c3be-41f2-b384-1febbc585bea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce98eba9-f150-4aec-8d9c-7d1729704115",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_ID = sp.eos_id()\n",
    "@torch.no_grad()\n",
    "def generate_answer_from_ids(prompt_ids, max_new_tokens=300, temperature=0.7, top_p=0.95, repetition_penalty=1.2):\n",
    "    input_ids = torch.tensor([prompt_ids], dtype=torch.long).to(DEVICE)\n",
    "    generated = input_ids.clone()  # copy for tracking\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        if generated.shape[1] >= 512:\n",
    "            generated = generated[:, -512:]\n",
    "            \n",
    "        logits = model(generated)\n",
    "        next_logits = logits[:, -1, :]\n",
    "        \n",
    "        # Repetition penalty\n",
    "        if repetition_penalty > 1.0:\n",
    "            recent = generated[0, -128:].tolist()\n",
    "            for tid in set(recent):\n",
    "                next_logits[0, tid] /= repetition_penalty\n",
    "        \n",
    "        # Temperature + top-p\n",
    "        next_logits = next_logits / temperature\n",
    "        probs = torch.softmax(next_logits, dim=-1)\n",
    "        \n",
    "        # Nucleus sampling\n",
    "        sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
    "        cum_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "        mask = cum_probs > top_p\n",
    "        mask[..., 1:] = mask[..., :-1].clone()\n",
    "        mask[..., 0] = False\n",
    "        sorted_probs[mask] = 0.0\n",
    "        if sorted_probs.sum() > 0:\n",
    "            sorted_probs /= sorted_probs.sum()\n",
    "        \n",
    "        next_token = torch.multinomial(sorted_probs, num_samples=1)\n",
    "        next_token = sorted_idx.gather(-1, next_token)\n",
    "        \n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "        \n",
    "        if next_token.item() == sp.eos_id():\n",
    "            break\n",
    "    \n",
    "    # Sirf NEW generated tokens decode karo (prompt ke baad se)\n",
    "    prompt_len = len(prompt_ids)\n",
    "    answer_ids = generated[0, prompt_len:].tolist()\n",
    "    return sp.decode(answer_ids).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "894785a6-544f-45cd-8679-fbedfa6c7731",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompt_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prompt_ids  \u001b[38;5;66;03m# No BOS if causing issues, ya sp.bos_id() last mein try\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Generate settings\u001b[39;00m\n\u001b[0;32m     12\u001b[0m answer \u001b[38;5;241m=\u001b[39m generate_answer_from_ids(\n\u001b[1;32m---> 13\u001b[0m     \u001b[43mprompt_ids\u001b[49m,\n\u001b[0;32m     14\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m250\u001b[39m,\n\u001b[0;32m     15\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,\n\u001b[0;32m     16\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m,\n\u001b[0;32m     17\u001b[0m     repetition_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.25\u001b[39m\n\u001b[0;32m     18\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'prompt_ids' is not defined"
     ]
    }
   ],
   "source": [
    "# build_prompt_ids ko simple bana\n",
    "def build_prompt_ids(context_ids, question):\n",
    "    instruction = \"संदर्भ के आधार पर हिंदी में सटीक उत्तर दो।\"\n",
    "    prompt_ids = (\n",
    "        sp.encode(instruction + \"\\n\\nसंदर्भ:\\n\") +\n",
    "        context_ids[:250] +  # Hard limit 250 tokens context\n",
    "        sp.encode(\"\\nप्रश्न: \" + question + \"\\nउत्तर:\\n\")\n",
    "    )\n",
    "    return prompt_ids  # No BOS if causing issues, ya sp.bos_id() last mein try\n",
    "\n",
    "# Generate settings\n",
    "answer = generate_answer_from_ids(\n",
    "    prompt_ids,\n",
    "    max_new_tokens=250,\n",
    "    temperature=0.9,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf70d1b-6040-4438-9169-1a1745c131ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
